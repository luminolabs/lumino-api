This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-03-05T22:25:44.453Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
core/
  authentication.py
  common.py
  config_manager.py
  constants.py
  cryptography.py
  database.py
  exceptions.py
  scheduler_client.py
  storage.py
  stripe_client.py
  utils.py
models/
  api_key.py
  base_model.py
  billing_credit.py
  dataset.py
  fine_tuned_model.py
  fine_tuning_job_detail.py
  fine_tuning_job.py
  usage.py
  user.py
queries/
  api_keys.py
  billing.py
  common.py
  datasets.py
  fine_tuned_models.py
  fine_tuning.py
  models.py
  usage.py
  users.py
routes/
  api_keys.py
  auth0.py
  billing.py
  datasets.py
  fine_tuning.py
  models.py
  usage.py
  users.py
schemas/
  api_key.py
  billing.py
  common.py
  dataset.py
  fine_tuning.py
  model.py
  usage.py
  user.py
services/
  api_key.py
  auth0.py
  billing.py
  dataset.py
  fine_tuned_model.py
  fine_tuning.py
  model.py
  usage.py
  user.py
tasks/
  api_key_cleanup.py
  job_status_updater.py
  model_cleanup.py
main.py

================================================================
Repository Files
================================================================

================
File: core/authentication.py
================
from fastapi import Depends, Header, Request
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.constants import UserStatus, ApiKeyStatus
from app.core.database import get_db
from app.core.exceptions import (
    InvalidApiKeyError,
    UnauthorizedError,
    InvalidUserSessionError,
    ForbiddenError
)
from app.core.stripe_client import create_stripe_customer
from app.core.utils import setup_logger
from app.models.user import User
from app.queries import api_keys as api_key_queries
from app.queries import users as user_queries
from app.queries.common import now_utc, make_naive

logger = setup_logger(__name__)


async def get_api_key(x_api_key: str | None = Header(None)) -> str | None:
    """Get API key from request header."""
    return x_api_key


async def get_user_from_api_key(db: AsyncSession, api_key: str) -> User:
    """
    Get user associated with an API key.

    Args:
        api_key: The API key to verify
        db: Database session

    Returns:
        Associated user

    Raises:
        InvalidApiKeyError: If API key is invalid
    """
    # Get API key from database
    db_api_key = await api_key_queries.get_api_key_by_prefix(db, api_key[:8])

    if not db_api_key or db_api_key.status != ApiKeyStatus.ACTIVE or db_api_key.expires_at < make_naive(now_utc()):
        raise InvalidApiKeyError(
            f"API key not found, expired, or revoked: {api_key[:8]}...",
            logger
        )

    if not db_api_key.verify_key(api_key):
        raise InvalidApiKeyError(
            f"Can't verify API key: {api_key[:8]}...",
            logger
        )

    user = await user_queries.get_user_by_id(db, db_api_key.user_id)
    logger.info(
        f"User authenticated via API key: {user}, "
        f"API key: {api_key[:8]}..."
    )
    return user


async def get_session_user(
        request: Request,
        db: AsyncSession = Depends(get_db)
) -> User | None:
    """Get user from session."""
    user = request.session.get('user')
    if user:
        db_user = await user_queries.get_user_by_email(db, user['email'])
        if db_user and db_user.status == UserStatus.ACTIVE:
            return db_user
    return None


async def get_current_active_user(
        user: User = Depends(get_session_user),
        api_key: str | None = Depends(get_api_key),
        db: AsyncSession = Depends(get_db)
) -> User:
    """
    Get current active user from session or API key.

    Args:
        user: User from session
        api_key: API key from header
        db: Database session

    Returns:
        Current active user

    Raises:
        UnauthorizedError: If no valid authentication
        InvalidUserSessionError: If user session invalid
    """
    if not api_key and not user:
        raise UnauthorizedError(
            "No x_api_key header or user session found",
            logger
        )

    if api_key:
        user = await get_user_from_api_key(db, api_key)

    if not user:
        raise InvalidUserSessionError(
            "User session not found, or user not found, or inactive",
            logger
        )

    # Ensure user has Stripe customer ID
    if not user.stripe_customer_id:
        await create_stripe_customer(db, user)

    return user


def admin_required(user: User = Depends(get_current_active_user)) -> User:
    """Verify user is admin."""
    if not user.is_admin:
        raise ForbiddenError(
            "Admin access required to perform this action",
            logger
        )
    return user

================
File: core/common.py
================
import re
from datetime import date, datetime

from app.core.exceptions import BadRequestError


def parse_date(date_str: str) -> date | None:
    """
    Parse a date string in the format YYYY-MM-DD.

    Args:
        date_str (str): The date string to parse.
    Returns:
        date: The parsed date.
    """
    if not date_str:
        return None
    try:
        return datetime.strptime(date_str, "%Y-%m-%d").date()
    except ValueError:
        raise BadRequestError(f"Invalid date format, use YYYY-MM-DD. Example: 2022-12-29; got: {date_str}")


def parse_datetime(datetime_str: str) -> datetime | None:
    """
    Parse a datetime string in the format %Y-%m-%dT%H:%M:%SZ.

    Args:
        datetime_str (str): The datetime string to parse.
    Returns:
        datetime: The parsed datetime.
    """
    if not datetime_str:
        return None
    try:
        return datetime.strptime(datetime_str, "%Y-%m-%dT%H:%M:%SZ")
    except ValueError:
        raise BadRequestError(f"Invalid datetime format, use %Y-%m-%dT%H:%M:%SZ. "
                              f"Example: 2022-12-29T12:00:00Z; got: {datetime_str}")


def sanitize_filename(filename: str) -> str:
    """
    Sanitize the filename to use only lowercase letters, numbers, hyphens, and underscores.

    Args:
        filename (str): The original filename.

    Returns:
        str: The sanitized filename.

    Raises:
        BadRequestError: If the filename cannot be sanitized to a valid format.
    """
    if not filename:
        raise BadRequestError("Filename cannot be empty")

    # Get file extension if it exists
    parts = filename.rsplit('.', 1)
    name = parts[0]
    extension = f".{parts[1].lower()}" if len(parts) > 1 else ""

    # Convert to lowercase
    sanitized = name.lower()

    # Replace spaces and other special characters with underscore
    sanitized = re.sub(r'[^a-z0-9-]', '_', sanitized)

    # Remove consecutive underscores/hyphens
    sanitized = re.sub(r'[-_]{2,}', '_', sanitized)

    # Remove leading/trailing underscores/hyphens
    sanitized = sanitized.strip('_-')

    # Recombine with extension
    result = sanitized + extension

    # Check if we have a valid filename
    if not sanitized:
        raise BadRequestError(
            f"Filename must contain at least one letter or number; got: {filename}"
        )

    if len(result) > 255:
        raise BadRequestError(
            f"Filename is too long (max 255 characters); got {len(result)} characters"
        )

    return result.strip()

================
File: core/config_manager.py
================
import os

import yaml


class ConfigManager:
    """
    Manages loading application configurations from YAML files, with support for
    separate environments and overrides using environment variables.
    """

    def __init__(self):
        """
        Initializes the configuration manager.
        """
        # Configuration folder path
        self.app_configs_path = os.environ.get('CAPI_CONF_PATH', 'app-configs')
        # Get the application environment
        # We aren't setting a default value on the `get()` method below
        # because we want a default value if:
        # 1. `CAPI_ENV` is set, but empty
        # 2. `CAPI_ENV` is not set
        # Setting a default here won't cover 1.
        # So, we set the default below instead in a
        # different way.
        self.env_name = os.environ.get('CAPI_ENV')
        # Set default application environment to `local`
        if not self.env_name:
            self.env_name = 'local'
        # Load configuration
        self.loaded_config = self.load()

    @property
    def database_url(self) -> str:
        """
        Constructs and returns the database URL.

        :return: The constructed database URL.
        """
        return f"postgresql+asyncpg://{self.db_user}:{self.db_pass}@{self.db_host}:{self.db_port}/{self.db_name}"

    def load(self) -> dict:
        """
        Loads configuration from the base directory, with environment-specific overrides
        and the ability to override values with environment variables.

        :return: The merged configuration dictionary.
        """

        # Default configuration
        default_file = os.path.join(self.app_configs_path, 'default.yml')
        # Environment specific configuration
        env_file = os.path.join(self.app_configs_path, f'{self.env_name}.yml')

        loaded_config = {}
        # Load configuration from config files
        for file in (default_file, env_file):
            if os.path.exists(file):
                with open(file, 'r') as f:
                    loaded_config.update(yaml.safe_load(f))

        # Override configuration with environment variables
        for key, value in loaded_config.items():
            env_var_name = f'CAPI_{key.upper()}'
            if env_var_name in os.environ:
                env_var_value = os.environ[env_var_name]
                if not env_var_value:
                    continue
                if is_truthy(env_var_value):
                    env_var_value = True
                elif is_falsy(env_var_value):
                    env_var_value = False
                loaded_config[key] = env_var_value

        # Set configuration as environment variables,
        # for libraries like google (ex. GOOGLE_APPLICATION_CREDENTIALS)
        # that look at env vars for configuration
        for key, value in loaded_config.items():
            os.environ[key] = str(value)

        return loaded_config

    def __getattr__(self, attr):
        return self.loaded_config[attr]


def is_truthy(value) -> bool:
    """
    Checks if the given value is truthy
    :param value: Value to check
    :return: True if the given value is truthy
    """
    if isinstance(value, str):
        value = value.lower()
    return any(value == x for x in (True, 'true', '1', 'yes'))


def is_falsy(value) -> bool:
    """
    Checks if the given value is falsy
    :param value: Value to check
    :return: True if the given value is falsy
    """
    if isinstance(value, str):
        value = value.lower()
    return any(value == x for x in (False, 'false', '0', 'no'))


config = ConfigManager()

================
File: core/constants.py
================
from enum import Enum


# Enumeration for user account statuses
class UserStatus(str, Enum):
    ACTIVE = "ACTIVE"  # User account is active and can be used
    INACTIVE = "INACTIVE"  # User account is deactivated and cannot be used


# Enumeration for API key statuses
class ApiKeyStatus(str, Enum):
    ACTIVE = "ACTIVE"  # API key is active and can be used for authentication
    EXPIRED = "EXPIRED"  # API key has expired and is no longer valid
    REVOKED = "REVOKED"  # API key has been manually revoked by the user or admin


# Enumeration for dataset statuses
class DatasetStatus(str, Enum):
    UPLOADED = "UPLOADED"  # Dataset has been uploaded but not yet validated
    VALIDATED = "VALIDATED"  # Dataset has been validated and is ready for use
    ERROR = "ERROR"  # There was an error processing or validating the dataset
    DELETED = "DELETED"  # Dataset has been marked as deleted


# Enumeration for fine-tuning job statuses
class FineTuningJobStatus(str, Enum):
    NEW = "NEW"  # Job has been created but not yet started
    QUEUED = "QUEUED"  # Job is queued and waiting to start
    RUNNING = "RUNNING"  # Job is currently running
    STOPPING = "STOPPING"  # Job is in the process of being stopped
    STOPPED = "STOPPED"  # Job has been stopped by the user or system
    COMPLETED = "COMPLETED"  # Job has completed successfully
    FAILED = "FAILED"  # Job has failed to complete
    DELETED = "DELETED"  # Job has been marked as deleted


# Enumeration for compute providers
class ComputeProvider(str, Enum):
    GCP = "GCP"
    LUM = "LUM"


# Enumeration for fine-tuning job types
class FineTuningJobType(str, Enum):
    FULL = "FULL"  # Full fine-tuning job
    LORA = "LORA"  # Low-resource fine-tuning job
    QLORA = "QLORA"  # Quantized low-resource fine-tuning job


# Enumeration for base model statuses
class BaseModelStatus(str, Enum):
    ACTIVE = "ACTIVE"  # Base model is available for use
    INACTIVE = "INACTIVE"  # Base model is deactivated and cannot be used
    DEPRECATED = "DEPRECATED"  # Base model is no longer supported or recommended for use


# Enumeration for fine-tuned model statuses
class FineTunedModelStatus(str, Enum):
    ACTIVE = "ACTIVE"  # Fine-tuned model is available for use
    DELETED = "DELETED"  # Fine-tuned model has been marked as deleted


class UsageUnit(str, Enum):
    """
    Enum for the unit of the available usage units.
    """
    TOKEN = "TOKEN"


class ServiceName(str, Enum):
    """
    Enum for the name of the available services.
    """
    FINE_TUNING_JOB = "FINE_TUNING_JOB"


class BillingTransactionType(str, Enum):
    """
    Enum for the type of billing transaction.
    """
    MANUAL_ADJUSTMENT = "MANUAL_ADJUSTMENT"
    NEW_USER_CREDIT = "NEW_USER_CREDIT"
    FINE_TUNING_JOB = "FINE_TUNING_JOB"
    STRIPE_CHECKOUT = "STRIPE_CHECKOUT"

================
File: core/cryptography.py
================
import secrets

from bcrypt import hashpw, checkpw, gensalt

from app.core.config_manager import config
from app.core.utils import setup_logger

# Set up logger
logger = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


def verify_password(plain_password: str, hashed_password: str) -> bool:
    """
    Verify a password against a hash.

    Args:
        plain_password (str): The plain text password.
        hashed_password (str): The hashed password.
    Returns:
        bool: Whether the password is correct.
    """
    return checkpw(plain_password.encode('utf-8'), hashed_password.encode('utf-8'))


def get_password_hash(password: str) -> str:
    """
    Generate a hash for a password.

    Args:
        password (str): The password to hash.
    Returns:
        str: The hashed password.
    """
    return hashpw(password.encode('utf-8'), gensalt()).decode('utf-8')


def generate_api_key() -> tuple[str, str]:
    """
    Generate a new API key and its hash.

    Returns:
        tuple[str, str]: The API key and its hashed value.
    """
    api_key = secrets.token_urlsafe(32)
    key_hash = get_password_hash(api_key)
    logger.info("Generated new API key")
    return api_key, key_hash

================
File: core/database.py
================
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import sessionmaker

from app.core.config_manager import config

# Create the database engine
engine = create_async_engine(config.database_url, echo=config.sqlalchemy_log_all)
AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
Base = declarative_base()


async def get_db():
    """
    Async context manager to handle database sessions.
    This function can be used as a FastAPI dependency to get a database session.
    """
    async with AsyncSessionLocal() as db:
        try:
            # Yield the database session to the function that depends on it
            yield db
        finally:
            # Ensure the session is closed after the function finishes
            await db.close()

================
File: core/exceptions.py
================
import logging
from logging import Logger
from typing import Optional

from fastapi import HTTPException, Request
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from sqlalchemy.exc import SQLAlchemyError

from app.core.config_manager import config
from app.core.utils import setup_logger

# Set up logger
logger_ = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


class AppException(HTTPException):
    """Base exception for application-specific errors."""

    def __init__(self, status_code: int, detail: str, logger: Optional[Logger] = None):
        """
        Initialize the exception.

        Args:
            status_code (int): The HTTP status code.
            detail (str): The detail message.
            logger (Optional[Logger]): The logger instance to use, if any.
        """
        super().__init__(status_code=status_code, detail=detail)
        if logger:
            self.log(logger)

    def log(self, logger: Logger):
        """
        Log the exception using the provided logger.

        Args:
            logger (Logger): The logger instance to use.
        """
        level = logging.ERROR if self.status_code >= 500 else logging.WARNING
        logger.log(level, self.detail)

    def __str__(self) -> str:
        """
        Return the exception message when cast to a string.

        Returns:
            str: The detail message of the exception.
        """
        return self.detail


class NotFoundError(AppException):
    """Exception raised when a requested resource is not found."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(status_code=404, detail=detail, logger=logger)


class UnauthorizedError(AppException):
    """Exception raised when authentication fails."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(status_code=401, detail=detail, logger=logger)


class ForbiddenError(AppException):
    """Exception raised when a user doesn't have permission to access a resource."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(status_code=403, detail=detail, logger=logger)


class BadRequestError(AppException):
    """Exception raised when the request is malformed or invalid."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(status_code=422, detail=detail, logger=logger)


class ServerError(AppException):
    """Exception raised when an unexpected application error occurs."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(status_code=500, detail=detail, logger=logger)


class PaymentNeededError(AppException):
    """Exception raised when a payment is needed."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(status_code=402, detail=detail, logger=logger)


# Authentication exceptions


class InvalidApiKeyError(UnauthorizedError):
    """Exception raised when an invalid API key is provided."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


class InvalidUserSessionError(UnauthorizedError):
    """Exception raised when an invalid token is provided."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


# User exceptions


class UserNotFoundError(NotFoundError):
    """Exception raised when a requested user is not found."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


class EmailAlreadyExistsError(BadRequestError):
    """Exception raised when attempting to register with an email that's already in use."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


# API key exceptions


class ApiKeyAlreadyExistsError(BadRequestError):
    """Exception raised when there's an error creating an API key."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


class ApiKeyNotFoundError(NotFoundError):
    """Exception raised when a requested API key is not found."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


# Dataset exceptions


class DatasetAlreadyExistsError(BadRequestError):
    """Exception raised when there's an error creating a dataset."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


class DatasetNotFoundError(NotFoundError):
    """Exception raised when a requested dataset is not found."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


# Model exceptions


class BaseModelNotFoundError(NotFoundError):
    """Exception raised when a requested base model is not found."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


class FineTunedModelNotFoundError(NotFoundError):
    """Exception raised when a requested fine-tuned model is not found."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


# Fine-Tuning exceptions


class FineTuningJobNotFoundError(NotFoundError):
    """Exception raised when a requested fine-tuning job is not found."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


class FineTuningJobAlreadyExistsError(BadRequestError):
    """Exception raised when a fine-tuning job with the same name already exists."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


class FineTuningJobCreationError(ServerError):
    """Exception raised when there's an error creating a fine-tuning job."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


class FineTuningJobRefreshError(ServerError):
    """Exception raised when there's an error refreshing fine-tuning job details."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


class FineTuningJobCancellationError(ServerError):
    """Exception raised when there's an error stopping a fine-tuning job."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


# Other exceptions


class StripeCheckoutSessionCreationError(ServerError):
    """Exception raised when there's an error creating a Stripe checkout session."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


class StorageError(ServerError):
    """Exception for storage operations failures."""

    def __init__(self, detail: str, logger: Optional[Logger] = None):
        super().__init__(detail, logger)


# Exception handlers


# noinspection PyUnusedLocal
async def app_exception_handler(request: Request, exc: AppException):
    """Handler for application-specific exceptions."""
    return JSONResponse(
        status_code=exc.status_code,
        content={"status": exc.status_code, "message": exc.detail},
    )


# noinspection PyUnusedLocal
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """Handler for request validation errors."""
    return JSONResponse(
        status_code=422,
        content={"status": 422, "message": exc.errors()},
    )


# noinspection PyUnusedLocal
async def sqlalchemy_exception_handler(request: Request, exc: SQLAlchemyError):
    """Handler for SQLAlchemy database errors."""
    logger_.exception(f"An unexpected database error occurred: {exc}, {exc.with_traceback(exc.__traceback__)}")
    return JSONResponse(
        status_code=500,
        content={"status": 500, "message": "An unexpected database error occurred"},
    )


# noinspection PyUnusedLocal
async def generic_exception_handler(request: Request, exc: Exception):
    """Handler for generic, uncaught exceptions."""
    logger_.exception(f"An unexpected error occurred: {exc}, {exc.with_traceback(exc.__traceback__)}")
    return JSONResponse(
        status_code=500,
        content={"status": 500, "message": "An unexpected error occurred"},
    )

================
File: core/scheduler_client.py
================
from typing import List, Dict, Any
from uuid import UUID

import aiohttp
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config_manager import config
from app.core.constants import FineTuningJobStatus
from app.core.exceptions import (
    FineTuningJobCreationError,
    FineTuningJobRefreshError,
    FineTuningJobCancellationError
)
from app.core.utils import setup_logger
from app.queries import fine_tuning as ft_queries
from app.services.dataset import get_dataset_bucket

logger = setup_logger(__name__)

INTERNAL_API_URL = config.scheduler_zen_url


async def start_fine_tuning_job(db: AsyncSession, job_id: UUID, user_id: UUID) -> None:
    """Start a fine-tuning job via scheduler."""
    if not config.run_with_scheduler:
        logger.info("Scheduler API is disabled")
        return

    # Get job with all required information
    job_info = await ft_queries.get_job_with_details_full(db, job_id, user_id)
    if not job_info:
        raise FineTuningJobCreationError(
            f"Failed to find job: {job_id}",
            logger
        )

    job, dataset, base_model, job_detail = job_info

    # Extract cluster configuration
    use_lora = job_detail.parameters.get("use_lora", True)
    use_qlora = job_detail.parameters.get("use_qlora", False)

    if not use_lora:
        use_qlora = False

    cluster_config_name = (
        f"{'q' if use_qlora else ''}"
        f"{'lora' if use_lora else 'full'}"
    )

    cluster_config = base_model.cluster_config.get(cluster_config_name)
    num_gpus = cluster_config.get("num_gpus")
    gpu_type = cluster_config.get("gpu_type")

    # Prepare scheduler payload
    payload = {
        "job_id": str(job_id),
        "workflow": "torchtunewrapper",
        "args": {
            "job_config_name": base_model.name,
            "dataset_id": (
                f"gs://{get_dataset_bucket()}/{user_id}/"
                f"{dataset.file_name}"
            ),
            "batch_size": job_detail.parameters.get("batch_size", 2),
            "shuffle": job_detail.parameters.get("shuffle", True),
            "num_epochs": job_detail.parameters.get("num_epochs", 1),
            "use_lora": use_lora,
            "use_qlora": use_qlora,
            "num_gpus": num_gpus,
        },
        "gpu_type": gpu_type,
        "num_gpus": num_gpus,
        "user_id": str(job.user_id),
        "keep_alive": False,
    }

    if config.env_name in ("dev", "prod"):
        payload["args"]["override_env"] = config.env_name

    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                    f"{INTERNAL_API_URL}/jobs/{job.provider.value.lower()}",
                    json=payload
            ) as response:
                if response.status == 200:
                    logger.info(f"Started fine-tuning job: {job_id}")
                elif response.status == 422:
                    error_data = await response.json()
                    raise FineTuningJobCreationError(
                        f"Failed to start job {job_id}: {error_data['message']}",
                        logger
                    )
                else:
                    raise FineTuningJobCreationError(
                        f"Failed to start job {job_id}: {await response.text()}",
                        logger
                    )

    except Exception as e:
        # Update job status to failed
        job.status = FineTuningJobStatus.FAILED
        await db.commit()
        raise FineTuningJobCreationError(
            f"Failed to start job {job_id}: {str(e)}",
            logger
        )


async def fetch_job_details(
        user_id: UUID,
        job_ids: List[UUID]
) -> List[Dict[str, Any]]:
    """
    Get job status updates from scheduler.

    Args:
        user_id: User ID
        job_ids: List of job IDs to check

    Returns:
        List of job status details

    Raises:
        FineTuningJobRefreshError: If update fails
    """
    if not config.run_with_scheduler:
        logger.info("Scheduler API is disabled")
        return []

    user_id_str = str(user_id)
    job_ids_str = [str(job_id) for job_id in job_ids]

    async with aiohttp.ClientSession() as session:
        async with session.post(
                f"{INTERNAL_API_URL}/jobs/get_by_user_and_ids",
                json={"user_id": user_id_str, "job_ids": job_ids_str}
        ) as response:
            if response.status == 200:
                return await response.json()
            else:
                raise FineTuningJobRefreshError(
                    f"Error refreshing job statuses: {await response.text()}"
                )


async def stop_fine_tuning_job(job_id: UUID, user_id: UUID) -> None:
    """
    Stop a running fine-tuning job.

    Args:
        job_id: Job ID to stop
        user_id: User ID

    Raises:
        FineTuningJobCancellationError: If stop request fails
    """
    if not config.run_with_scheduler:
        logger.info("Scheduler API is disabled")
        return

    async with aiohttp.ClientSession() as session:
        async with session.post(
                f"{INTERNAL_API_URL}/jobs/gcp/stop/{job_id}/{user_id}"
        ) as response:
            if response.status == 200:
                logger.info(f"Requested stop for job: {job_id}")
                return await response.json()
            elif response.status == 404:
                raise FineTuningJobCancellationError(
                    f"Job not found or not running: {job_id}",
                    logger
                )
            else:
                raise FineTuningJobCancellationError(
                    f"Failed to stop job {job_id}: {await response.text()}",
                    logger
                )

================
File: core/storage.py
================
import datetime
import os
from uuid import UUID

from aiohttp import ClientSession, ClientResponseError
from fastapi import UploadFile
from gcloud.aio.storage import Storage

from app.core.config_manager import config
from app.core.exceptions import ServerError, StorageError
from app.core.utils import setup_logger

logger = setup_logger(__name__)


def handle_gcs_error(e: ClientResponseError, file_path: str) -> None:
    """Handle Google Cloud Storage errors."""
    if e.status == 404:
        logger.warning(f"File not found in GCS: {file_path}")
    elif e.status == 400 and "invalid_grant" in e.message:
        raise ServerError(
            "Authentication error: Are you authenticated with Google Cloud SDK?",
            logger
        )
    else:
        raise StorageError(f"GCS operation failed: {str(e)}", logger)


async def upload_file(bucket: str, path: str, file: UploadFile, user_id: UUID) -> str:
    """
    Upload file to Google Cloud Storage.

    Args:
        bucket: GCS bucket name
        path: Storage path
        file: File to upload
        user_id: User ID for file path

    Returns:
        Uploaded file name

    Raises:
        StorageError: If upload fails
    """
    # Create file path with timestamp prefix
    current_datetime = datetime.datetime.now()
    datetime_str = current_datetime.strftime("%Y-%m-%d_%H-%M-%S")
    file_name = f"{datetime_str}_{file.filename}"
    file_path = os.path.join(path, str(user_id), file_name)

    try:
        # Read file contents
        contents = await file.read()

        # Upload to GCS
        async with ClientSession() as session:
            storage = Storage(session=session)
            await storage.upload(
                bucket=bucket,
                object_name=file_path,
                file_data=contents,
                content_type=file.content_type
            )

        logger.info(f"Uploaded file: {file_path} for user: {user_id}")
        return file_name

    except ClientResponseError as e:
        handle_gcs_error(e, file_path)
        return file_name  # Only reached for 404 errors
    except Exception as e:
        raise StorageError(f"Failed to upload file: {str(e)}", logger)


async def delete_file(bucket: str, path: str, file_name: str, user_id: UUID) -> None:
    """
    Delete file from Google Cloud Storage.

    Args:
        bucket: GCS bucket name
        path: Storage path
        file_name: File name to delete
        user_id: User ID for file path

    Raises:
        StorageError: If deletion fails
    """
    file_path = os.path.join(path, str(user_id), file_name)

    try:
        async with ClientSession() as session:
            storage = Storage(session=session)
            await storage.delete(
                bucket=bucket,
                object_name=file_path
            )

        logger.info(f"Deleted file: {file_path} for user: {user_id}")

    except ClientResponseError as e:
        handle_gcs_error(e, file_path)
    except Exception as e:
        raise StorageError(f"Failed to delete file: {str(e)}", logger)

================
File: core/stripe_client.py
================
from math import ceil

import stripe
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config_manager import config
from app.core.exceptions import ServerError
from app.core.utils import setup_logger
from app.models.user import User

# Set up logger
logger = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


async def create_stripe_customer(db: AsyncSession, user: User) -> stripe.Customer | None:
    """
    Creates a new customer in Stripe.
    """
    try:
        stripe_customer = None

        # Check if the user already has a Stripe customer ID
        if user.stripe_customer_id:
            return stripe.Customer.retrieve(user.stripe_customer_id)

        # Search if the customer already exists in Stripe
        stripe_customers = stripe.Customer.list(email=user.email)
        if stripe_customers:
            stripe_customer = stripe_customers.data[0]

        # Create the customer in Stripe
        if not stripe_customer:
            stripe_customer = stripe.Customer.create(
                email=user.email,
                name=user.name,
            )

        # Update the user with the Stripe customer ID
        user.stripe_customer_id = stripe_customer.id
        await db.commit()
        await db.refresh(user)  # Refreshes the updated_at field
        return stripe_customer
    except stripe.error.StripeError as e:
        print(f"Stripe error for user: {user.id} message: {e.user_message}")
        return None


def create_stripe_checkout_session(
        user: User,
        amount_dollars: int,
        success_url: str,
        cancel_url: str
) -> stripe.checkout.Session:
    """
    Create a Stripe Checkout Session for adding credits.
    Generates a URL that the user can visit to add credits to their account.
    """
    try:
        # Create the Checkout Session
        session = stripe.checkout.Session.create(
            payment_method_types=['card'],
            customer=user.stripe_customer_id,
            line_items=[{
                'price_data': {
                    'currency': 'usd',
                    'product_data': {
                        'name': 'Lumino Credits',
                    },
                    'unit_amount': ceil(amount_dollars * 100),  # Convert dollars to cents
                },
                'quantity': 1,
            }],
            mode='payment',
            success_url=success_url,
            cancel_url=cancel_url,
            client_reference_id=user.id,
        )
        return session
    except Exception as e:
        raise ServerError(f"Error creating Stripe checkout session: {str(e)}", logger)


def create_stripe_billing_portal_session(user: User, success_url: str) -> stripe.billing_portal.Session:
    """
    Create a Customer Portal session.
    Generates a URL that the user can visit to manage their billing information.
    """
    # Create a Customer Portal session
    session = stripe.billing_portal.Session.create(
        customer=user.stripe_customer_id,
        return_url=success_url
    )
    return session


def stripe_charge_offline(user: User, amount: float) -> stripe.Invoice | None:
    """
    Charges a Stripe customer by creating an invoice and automatically charging their default payment method.
    """
    try:
        # Create the Invoice and Auto-Charge
        invoice = stripe.Invoice.create(
            customer=user.stripe_customer_id,
            auto_advance=True  # Auto-finalize and charge the customer immediately
        )
        # Create Invoice Items for each item in the list
        stripe.InvoiceItem.create(
            customer=user.stripe_customer_id,
            amount=ceil(amount * 100),  # Convert dollars to cents
            currency='usd',
            description='Lumino Credits (auto-charge)',
            invoice=invoice.id
        )
        # Finalize and Charge (if auto_advance is True, this will happen automatically)
        stripe.Invoice.finalize_invoice(invoice.id)
        stripe.Invoice.pay(invoice.id)
        return invoice

    except stripe.error.StripeError as e:
        # Handle Stripe errors
        print(f"Stripe error: {e.user_message}")
        return None

================
File: core/utils.py
================
import json
import logging
import os
import sys
from logging.handlers import TimedRotatingFileHandler
from typing import TypeVar, Any

from app.core.config_manager import config

T = TypeVar('T')


def setup_logger(name: str,
                 add_stdout: bool = True,
                 log_level: int = logging.INFO) -> logging.Logger:
    """
    Sets up a logger

    Args:
        name (str): The name of the logger.
        add_stdout (bool): Whether to log to stdout.
        log_level (int): The logging level.
    Returns:
        logging.Logger: The logger instance.
    """
    log_level = log_level or config.log_level
    log_format = logging.Formatter(f'{config.env_name} - %(asctime)s - %(message)s')

    # Log to stdout and to file
    os.makedirs(os.path.dirname(config.log_file), exist_ok=True)
    stdout_handler = logging.StreamHandler(sys.stdout)
    file_handler = TimedRotatingFileHandler(config.log_file, when="midnight", interval=1, backupCount=2)
    file_handler.suffix = "%Y%m%d"

    # Set the logger format
    stdout_handler.setFormatter(log_format)
    file_handler.setFormatter(log_format)

    # Configure logger
    logger = logging.getLogger(name)
    logger.setLevel(log_level)
    if add_stdout and config.log_stdout:
        logger.addHandler(stdout_handler)
    logger.addHandler(file_handler)
    return logger


def recursive_json_decode(data: Any) -> Any:
    """
    Recursively decode JSON strings into Python objects.
    """
    if isinstance(data, str):
        try:
            return recursive_json_decode(json.loads(data))
        except json.JSONDecodeError:
            return data
    elif isinstance(data, dict):
        return {key: recursive_json_decode(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [recursive_json_decode(item) for item in data]
    else:
        return data

================
File: models/api_key.py
================
from sqlalchemy import Column, String, DateTime, UUID, ForeignKey, UniqueConstraint, Enum
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func

from app.core.constants import ApiKeyStatus
from app.core.cryptography import verify_password
from app.core.database import Base


class ApiKey(Base):
    """
    Represents an API key that can be used to authenticate requests.

    Attributes:
        id (UUID): The unique identifier for the API key.
        created_at (DateTime): The timestamp when the API key was created.
        expires_at (DateTime): The timestamp when the API key expires.
        user_id (UUID): The ID of the user who owns this API key.
        status (ApiKeyStatus): The current status of the API key.
        name (str): The name of the API key.
        prefix (str): The prefix of the API key.
        key_hash (str): The hashed API key.

    Relationships:
        user (User): The user who owns this API key.
    """
    __tablename__ = "api_keys"

    # Columns
    id = Column(UUID, primary_key=True, server_default=func.gen_random_uuid(), nullable=False)
    created_at = Column(DateTime, server_default=func.now(), nullable=False)
    expires_at = Column(DateTime, nullable=False)
    user_id = Column(UUID, ForeignKey("users.id"), index=True)
    status = Column(Enum(ApiKeyStatus), nullable=False, default=ApiKeyStatus.ACTIVE)
    name = Column(String(255), nullable=False)
    prefix = Column(String(8), unique=True, index=True)
    key_hash = Column(String(255), nullable=False)

    # Relationships
    user = relationship("User", back_populates="api_keys")

    # Indexes
    __table_args__ = (
        UniqueConstraint('user_id', 'name', name='uq_api_key_user_id_name'),
    )

    def verify_key(self, api_key: str) -> bool:
        """
        Verify the provided API key against the stored hashed key.

        Args:
            api_key (str): The API key to verify.
        Returns:
            bool: True if the key is valid, False otherwise.
        """
        return verify_password(api_key, self.key_hash)

    def __repr__(self) -> str:
        return f"<ApiKey(id={self.id}, user_id={self.user_id}, name={self.name}, status={self.status})>"

================
File: models/base_model.py
================
from sqlalchemy import Column, String, UUID, JSON, Enum
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func

from app.core.constants import BaseModelStatus
from app.core.database import Base


class BaseModel(Base):
    """
    Represents a base language model available for fine-tuning.

    Attributes:
        id (UUID): The unique identifier for the base model.
        name (str): The name of the base model.
        description (str): A description of the base model.
        hf_url (str): The URL to the model on Hugging Face.
        status (BaseModelStatus): The current status of the base model.
        meta (JSON): Additional metadata stored as JSON.
        cluster_config (JSON): The cluster configuration for the base model; not exposed to the API.

    Relationships:
        fine_tuning_jobs (relationship): One-to-many relationship with FineTuningJob.
    """
    __tablename__ = "base_models"

    # Columns
    id = Column(UUID, primary_key=True, server_default=func.gen_random_uuid(), nullable=False)
    name = Column(String(255), nullable=False, unique=True, index=True)
    description = Column(String, nullable=True)
    hf_url = Column(String(255), nullable=False)
    status = Column(Enum(BaseModelStatus), nullable=False, default=BaseModelStatus.INACTIVE)
    meta = Column(JSON, nullable=True)
    cluster_config = Column(JSON, nullable=True)

    # Relationships
    fine_tuning_jobs = relationship("FineTuningJob", back_populates="base_model")

    def __repr__(self) -> str:
        return f"<BaseModel(id={self.id}, name={self.name}, status={self.status})>"

================
File: models/billing_credit.py
================
from sqlalchemy import Column, DateTime, UUID, ForeignKey, String, Enum, UniqueConstraint, Float
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func

from app.core.constants import BillingTransactionType
from app.core.database import Base


class BillingCredit(Base):
    """
    Represents a billing credits record for a user.

    Attributes:
        id (UUID): The unique identifier for the billing credits record.
        created_at (DateTime): The timestamp when the billing credits record was created.
        user_id (UUID): The ID of the user associated with these credits.
        credits (float): The amount of credits.
        transaction_id (str): The ID of the transaction associated with these credits.
        transaction_type (BillingTransactionType): The type of transaction associated with these

    Relationships:
        user (User): The user associated with these credits.
    """
    __tablename__ = "billing_credits"

    # Columns
    id = Column(UUID, primary_key=True, server_default=func.gen_random_uuid(), nullable=False)
    created_at = Column(DateTime, server_default=func.now(), nullable=False)
    user_id = Column(UUID, ForeignKey("users.id"), nullable=False, index=True)
    credits = Column(Float, nullable=False)
    transaction_id = Column(String(255), nullable=False)
    transaction_type = Column(Enum(BillingTransactionType), nullable=False)

    # Relationships
    user = relationship("User", back_populates="billing_credits")

    # Constraints
    __table_args__ = (
        UniqueConstraint("user_id", "transaction_id", "transaction_type",
                         name="uq_billing_credit_user_transaction"),
    )

    def __repr__(self) -> str:
        return f"<BillingCredit(id={self.id}, user_id={self.user_id}, credits={self.credits})>"

================
File: models/dataset.py
================
from sqlalchemy import Column, String, DateTime, UUID, BigInteger, JSON, ForeignKey, UniqueConstraint, Enum
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func

from app.core.constants import DatasetStatus
from app.core.database import Base


class Dataset(Base):
    """
    Represents a dataset used for fine-tuning language models.

    Attributes:
        id (UUID): The unique identifier for the dataset.
        created_at (DateTime): The timestamp when the dataset was created.
        user_id (UUID): The ID of the user who owns this dataset.
        status (DatasetStatus): The current status of the dataset.
        name (str): The name of the dataset.
        description (str | None): An optional description of the dataset.
        file_name (str): The file name of the dataset; we prefix with a timestamp and user ID to ensure uniqueness.
        file_size (int): The size of the dataset file in bytes.
        errors (dict | None): Any errors encountered during dataset processing.

    Relationships:
        user (User): The user who owns this dataset.
        fine_tuning_jobs (list[FineTuningJob]): The fine-tuning jobs using this dataset.
    """
    __tablename__ = "datasets"

    # Columns
    id = Column(UUID, primary_key=True, server_default=func.gen_random_uuid(), nullable=False)
    created_at = Column(DateTime, server_default=func.now(), nullable=False)
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now(), nullable=False)
    user_id = Column(UUID, ForeignKey("users.id"), nullable=False, index=True)
    status = Column(Enum(DatasetStatus), nullable=False, default=DatasetStatus.UPLOADED)
    name = Column(String(255), nullable=False)
    description = Column(String, nullable=True)
    file_name = Column(String(255), nullable=False)
    file_size = Column(BigInteger, nullable=False)
    errors = Column(JSON, nullable=True)

    # Relationships
    user = relationship("User", back_populates="datasets")
    fine_tuning_jobs = relationship("FineTuningJob", back_populates="dataset")

    # Indexes
    __table_args__ = (
        UniqueConstraint('user_id', 'name', name='uq_dataset_user_id_name'),
    )

    def __repr__(self) -> str:
        return f"<Dataset(id={self.id}, name={self.name}, user_id={self.user_id}, status={self.status})>"

================
File: models/fine_tuned_model.py
================
from sqlalchemy import Column, String, DateTime, UUID, JSON, ForeignKey, UniqueConstraint, Enum
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func

from app.core.constants import FineTunedModelStatus
from app.core.database import Base


class FineTunedModel(Base):
    """
    Represents a fine-tuned language model.

    Attributes:
        id (UUID): The unique identifier for the fine-tuned model.
        created_at (DateTime): The timestamp when the fine-tuned model was created.
        updated_at (DateTime): The timestamp when the fine-tuned model was last updated
        user_id (UUID): The ID of the user who created this fine-tuned model.
        fine_tuning_job_id (UUID): The ID of the fine-tuning job that created this model.
        name (str): The name of the fine-tuned model.
        status (FineTunedModelStatus): The current status of the fine-tuned model.
        artifacts (JSON): Information about model artifacts, stored as JSON.

    Relationships:
        user (User): The user who created this fine-tuned model.
        fine_tuning_job (FineTuningJob): The fine-tuning job that created this model.
    """
    __tablename__ = "fine_tuned_models"

    # Columns
    id = Column(UUID, primary_key=True, server_default=func.gen_random_uuid(), nullable=False)
    created_at = Column(DateTime, server_default=func.now(), nullable=False)
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now(), nullable=False)
    user_id = Column(UUID, ForeignKey("users.id"), nullable=False, index=True)
    fine_tuning_job_id = Column(UUID, ForeignKey("fine_tuning_jobs.id"), nullable=False)
    name = Column(String(255), nullable=False)
    status = Column(Enum(FineTunedModelStatus), nullable=False, default=FineTunedModelStatus.ACTIVE)
    artifacts = Column(JSON, nullable=False)

    # Relationships
    user = relationship("User", back_populates="fine_tuned_models")
    fine_tuning_job = relationship("FineTuningJob", back_populates="fine_tuned_model")

    # Constraints
    __table_args__ = (
        UniqueConstraint('user_id', 'name', name='uq_fine_tuned_model_user_id_name'),
    )

    def __repr__(self) -> str:
        return (f"<FineTunedModel(id={self.id}, name={self.name}, "
                f"user_id={self.user_id}, fine_tuning_job_id={self.fine_tuning_job_id})>")

================
File: models/fine_tuning_job_detail.py
================
from sqlalchemy import Column, JSON, UUID, ForeignKey
from sqlalchemy.orm import relationship

from app.core.database import Base


class FineTuningJobDetail(Base):
    """
    Represents detailed information for a fine-tuning job.

    Attributes:
        fine_tuning_job_id (UUID): The ID of the associated fine-tuning job.
        parameters (JSON): The parameters used for the fine-tuning job.
        metrics (JSON): The metrics collected during the fine-tuning process.
        timestamps (JSON): The timestamps recorded during the fine-tuning process.

    Relationships:
        fine_tuning_job (FineTuningJob): The fine-tuning job associated with these details.
    """
    __tablename__ = "fine_tuning_job_details"

    # Columns
    fine_tuning_job_id = Column(UUID, ForeignKey("fine_tuning_jobs.id"), primary_key=True)
    parameters = Column(JSON, nullable=False)  # Stores job parameters as JSON
    metrics = Column(JSON, nullable=True)  # Stores job metrics as JSON
    timestamps = Column(JSON, nullable=True)  # Stores job timestamps as JSON

    # Relationships
    fine_tuning_job = relationship("FineTuningJob", back_populates="details", uselist=False)

    def __repr__(self) -> str:
        return f"<FineTuningJobDetail(fine_tuning_job_id={self.fine_tuning_job_id})>"

================
File: models/fine_tuning_job.py
================
from sqlalchemy import Column, String, DateTime, UUID, Integer, BigInteger, ForeignKey, UniqueConstraint, Enum
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func

from app.core.constants import FineTuningJobStatus, FineTuningJobType, ComputeProvider
from app.core.database import Base


class FineTuningJob(Base):
    """
    Represents a fine-tuning job for a language model.

    Attributes:
        id (UUID): The unique identifier for the fine-tuning job.
        created_at (DateTime): The timestamp when the job was created.
        updated_at (DateTime): The timestamp when the job was last updated.
        user_id (UUID): The ID of the user who created the job.
        base_model_id (UUID): The ID of the base model used for fine-tuning.
        dataset_id (UUID): The ID of the dataset used for fine-tuning.
        status (FineTuningJobStatus): The current status of the job.
        name (str): The name of the fine-tuning job.
        type (FineTuningJobType): The type of fine-tuning job.
        provider (ComputeProvider): The compute provider used for fine-tuning.
        current_step (int): The current step of the fine-tuning process.
        total_steps (int): The total number of steps in the fine-tuning process.
        current_epoch (int): The current epoch of the fine-tuning process.
        total_epochs (int): The total number of epochs in the fine-tuning process.
        num_tokens (int): The number of tokens processed in the fine-tuning job.

    Relationships:
        user (User): The user who created the fine-tuning job.
        base_model (BaseModel): The base model used for fine-tuning.
        dataset (Dataset): The dataset used for fine-tuning.
        details (FineTuningJobDetail): The details of the fine-tuning job.
        fine_tuned_model (FineTunedModel): The fine-tuned model created by the job.
        usage_record (Usage): The usage record for the fine-tuning job.
    """
    __tablename__ = "fine_tuning_jobs"

    # Columns
    id = Column(UUID, primary_key=True, server_default=func.gen_random_uuid(), nullable=False)
    created_at = Column(DateTime, server_default=func.now(), nullable=False)
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now(), nullable=False)
    user_id = Column(UUID, ForeignKey("users.id"), nullable=False, index=True)
    base_model_id = Column(UUID, ForeignKey("base_models.id"), nullable=False)
    dataset_id = Column(UUID, ForeignKey("datasets.id"), nullable=False)
    status = Column(Enum(FineTuningJobStatus), nullable=False, default=FineTuningJobStatus.NEW)
    name = Column(String(255), nullable=False)
    type = Column(Enum(FineTuningJobType), nullable=False)
    provider = Column(Enum(ComputeProvider), nullable=False, default=ComputeProvider.GCP)
    current_step = Column(Integer, nullable=True)
    total_steps = Column(Integer, nullable=True)
    current_epoch = Column(Integer, nullable=True)
    total_epochs = Column(Integer, nullable=True)
    num_tokens = Column(BigInteger, nullable=True)

    # Relationships
    user = relationship("User", back_populates="fine_tuning_jobs")
    base_model = relationship("BaseModel", back_populates="fine_tuning_jobs")
    dataset = relationship("Dataset", back_populates="fine_tuning_jobs")
    details = relationship("FineTuningJobDetail", back_populates="fine_tuning_job", uselist=False)
    fine_tuned_model = relationship("FineTunedModel", back_populates="fine_tuning_job", uselist=False)
    usage_record = relationship("Usage", back_populates="fine_tuning_job", uselist=False)

    # Indexes
    __table_args__ = (
        UniqueConstraint('user_id', 'name', name='uq_fine_tuning_job_user_id_name'),
    )

    def __repr__(self) -> str:
        return (f"<FineTuningJob(id={self.id}, name={self.name}, user_id={self.user_id}, "
                f"base_model_id={self.base_model_id}, status={self.status})>")

================
File: models/usage.py
================
from sqlalchemy import Column, DateTime, UUID, ForeignKey, Enum, Integer, Float
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func

from app.core.constants import UsageUnit, ServiceName
from app.core.database import Base


class Usage(Base):
    """
    Represents a usage record for a service, such as fine-tuning a model.

    Attributes:
        id (UUID): The unique identifier for the usage record.
        created_at (DateTime): The timestamp when the usage record was created.
        user_id (UUID): The ID of the user who used the service.
        usage_amount (float): The amount of usage.
        usage_unit (UsageUnit): The unit of usage.
        cost (float): The cost of the usage.
        service_name (ServiceName): The name of the service.
        fine_tuning_job_id (UUID): The ID of the fine-tuning job associated with this usage record.

    Relationships:
        user (User): The user who used the service.
        fine_tuning_job (FineTuningJob): The fine-tuning job associated with this usage record.
    """
    __tablename__ = "usage"

    # Columns
    id = Column(UUID, primary_key=True, server_default=func.gen_random_uuid(), nullable=False)
    created_at = Column(DateTime, server_default=func.now(), nullable=False)
    user_id = Column(UUID, ForeignKey("users.id"), index=True, nullable=False)
    usage_amount = Column(Integer, nullable=False)
    usage_unit = Column(Enum(UsageUnit), nullable=False, default=UsageUnit.TOKEN)
    cost = Column(Float, nullable=False)
    service_name = Column(Enum(ServiceName), nullable=False, default=ServiceName.FINE_TUNING_JOB)
    fine_tuning_job_id = Column(UUID, ForeignKey("fine_tuning_jobs.id"), nullable=False, unique=True)

    # Relationships
    user = relationship("User", back_populates="usage_records")
    fine_tuning_job = relationship("FineTuningJob", back_populates="usage_record", uselist=False)

    def __repr__(self) -> str:
        return f"<Usage(id={self.id}, user_id={self.user_id}, service_name={self.service_name}, cost={self.cost})>"

================
File: models/user.py
================
from uuid import uuid4

from sqlalchemy import Column, String, DateTime, UUID, Index, Enum, Boolean, Float
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func

from app.core.constants import UserStatus
from app.core.database import Base


class User(Base):
    """
    Represents a user in the system.

    Attributes:
        id (UUID): The unique identifier for the user.
        created_at (DateTime): The timestamp when the user was created.
        updated_at (DateTime): The timestamp when the user was last updated.
        status (UserStatus): The current status of the user.
        name (str): The name of the user.
        email (str): The email address of the user.
        auth0_user_id (str): The Auth0 user ID.
        email_verified (bool): Whether the user's email is verified or not.
        is_admin (bool): Whether the user is an admin or not.
        credits_balance (float): The current credit balance of the user.

    Relationships:
        datasets (List[Dataset]): The datasets created by the user.
        fine_tuning_jobs (List[FineTuningJob]): The fine-tuning jobs created by the user.
        fine_tuned_models (List[FineTunedModel]): The fine-tuned models created by the user.
        api_keys (List[ApiKey]): The API keys owned by the user.
        usage_records (List[Usage]): The usage records for the user.
        billing_credits (List[BillingCredit]): The billing credits records for the user.
    """
    __tablename__ = "users"

    # Columns
    id = Column(UUID, primary_key=True, default=uuid4, nullable=False)
    created_at = Column(DateTime, server_default=func.now(), nullable=False)
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now(), nullable=False)
    status = Column(Enum(UserStatus), nullable=False, default=UserStatus.ACTIVE)
    name = Column(String(255), nullable=False)
    email = Column(String(255), nullable=False)
    auth0_user_id = Column(String(255), nullable=False)
    stripe_customer_id = Column(String(255), nullable=True)
    stripe_payment_method_id = Column(String(255), nullable=True)
    email_verified = Column(Boolean, nullable=False, default=False)
    is_admin = Column(Boolean, default=False, nullable=False)
    credits_balance = Column(Float, nullable=False, default=0)

    # Relationships
    datasets = relationship("Dataset", back_populates="user")
    fine_tuning_jobs = relationship("FineTuningJob", back_populates="user")
    fine_tuned_models = relationship("FineTunedModel", back_populates="user")
    api_keys = relationship("ApiKey", back_populates="user")
    usage_records = relationship("Usage", back_populates="user")
    billing_credits = relationship("BillingCredit", back_populates="user")

    # Indexes
    __table_args__ = (
        Index('idx_users_email', email, unique=True),
    )

    def __repr__(self) -> str:
        return f"<User(id={self.id}, name={self.name}, email={self.email}, is_admin={self.is_admin}, credits_balance={self.credits_balance})>"

================
File: queries/api_keys.py
================
from typing import List, Optional
from uuid import UUID

from sqlalchemy import select, func, update, and_
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.constants import ApiKeyStatus
from app.models.api_key import ApiKey
from app.queries.common import make_naive, now_utc


async def get_api_key_by_prefix(db: AsyncSession, prefix: str) -> Optional[ApiKey]:
    """Get an API key by its prefix."""
    result = await db.execute(
        select(ApiKey).where(
            ApiKey.prefix == prefix,
        )
    )
    return result.scalar_one_or_none()


async def get_api_key_by_name(db: AsyncSession, user_id: UUID, name: str) -> Optional[ApiKey]:
    """Get an API key by its name for a specific user."""
    result = await db.execute(
        select(ApiKey).where(ApiKey.user_id == user_id, ApiKey.name == name)
    )
    return result.scalar_one_or_none()


async def list_api_keys(db: AsyncSession, user_id: UUID, offset: int, limit: int) -> List[ApiKey]:
    """List API keys for a specific user with pagination."""
    result = await db.execute(
        select(ApiKey)
        .where(ApiKey.user_id == user_id)
        .order_by(ApiKey.created_at.desc())
        .offset(offset)
        .limit(limit)
    )
    return result.scalars().all()


async def count_api_keys(db: AsyncSession, user_id: UUID) -> int:
    """Count total API keys for a specific user."""
    result = await db.execute(
        select(func.count()).select_from(ApiKey).where(ApiKey.user_id == user_id)
    )
    return result.scalar_one()


async def mark_expired_keys(db: AsyncSession) -> int:
    """
    Mark expired API keys as EXPIRED.

    Args:
        db: Database session

    Returns:
        Number of keys marked as expired

    Note:
        This is an atomic operation that updates all expired keys at once
    """
    result = await db.execute(
        update(ApiKey)
        .where(
            and_(
                ApiKey.expires_at < make_naive(now_utc()),
                ApiKey.status == ApiKeyStatus.ACTIVE
            )
        )
        .values(
            status=ApiKeyStatus.EXPIRED
        )
        .returning(ApiKey.id)
    )

    updated_keys = result.scalars().all()
    count = len(updated_keys)

    return count

================
File: queries/billing.py
================
from datetime import date
from typing import List, Optional, Tuple
from uuid import UUID

from sqlalchemy import select, func, and_
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.constants import BillingTransactionType
from app.models.base_model import BaseModel
from app.models.billing_credit import BillingCredit
from app.models.fine_tuning_job import FineTuningJob


async def get_credit_record(
        db: AsyncSession,
        user_id: UUID,
        transaction_id: str,
        transaction_type: BillingTransactionType
) -> Optional[BillingCredit]:
    """
    Get a credit record by transaction details.

    Args:
        db: Database session
        user_id: User ID
        transaction_id: Transaction identifier
        transaction_type: Type of transaction

    Returns:
        Credit record if found, None otherwise
    """
    result = await db.execute(
        select(BillingCredit)
        .where(
            and_(
                BillingCredit.user_id == user_id,
                BillingCredit.transaction_id == transaction_id,
                BillingCredit.transaction_type == transaction_type
            )
        )
    )
    return result.scalar_one_or_none()


async def get_job_for_credits(
        db: AsyncSession,
        job_id: UUID,
        user_id: UUID
) -> Optional[Tuple[FineTuningJob, str]]:
    """Get a fine-tuning job and its base model name for credit calculation."""
    result = await db.execute(
        select(FineTuningJob, BaseModel.name.label('base_model_name'))
        .join(BaseModel, FineTuningJob.base_model_id == BaseModel.id)
        .where(
            FineTuningJob.id == job_id,
            FineTuningJob.user_id == user_id
        )
    )
    return result.first()


async def count_credit_history(
        db: AsyncSession,
        user_id: UUID,
        start_date: date,
        end_date: date
) -> int:
    """
    Count total number of credit history records for a user within a date range.

    Args:
        db: Database session
        user_id: User ID
        start_date: Start date
        end_date: End date

    Returns:
        Total count of credit history records
    """
    query = select(func.count()).select_from(BillingCredit).where(
        and_(
            BillingCredit.user_id == user_id,
            func.date(BillingCredit.created_at) >= start_date,
            func.date(BillingCredit.created_at) <= end_date
        )
    )

    result = await db.execute(query)
    return result.scalar_one()


async def get_credit_history(
        db: AsyncSession,
        user_id: UUID,
        start_date: date,
        end_date: date,
        offset: int,
        limit: int
) -> List[BillingCredit]:
    """
    Get credit history records for a user within a date range with pagination.

    Args:
        db: Database session
        user_id: User ID
        start_date: Start date
        end_date: End date
        offset: Pagination offset
        limit: Number of records to return

    Returns:
        List of credit history records
    """
    query = (
        select(BillingCredit)
        .where(
            and_(
                BillingCredit.user_id == user_id,
                func.date(BillingCredit.created_at) >= start_date,
                func.date(BillingCredit.created_at) <= end_date
            )
        )
        .order_by(BillingCredit.created_at.desc())
        .offset(offset)
        .limit(limit)
    )

    result = await db.execute(query)
    return result.scalars().all()

================
File: queries/common.py
================
from datetime import datetime, timezone
from typing import Tuple, List

from sqlalchemy import Select, select, func, Row
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.exceptions import BadRequestError
from app.core.utils import T
from app.schemas.common import Pagination


async def paginate_query(
        db: AsyncSession,
        query: Select,
        page: int,
        items_per_page: int
) -> Tuple[List[T], Pagination]:
    """
    Paginate a query and return the items and pagination object.

    Args:
        db (AsyncSession): The database session.
        query (Select): The SQLAlchemy query.
        page (int): The page number.
        items_per_page (int): The number of items per page.
    Returns:
        Tuple[List[T], Pagination]: A tuple of the items and pagination object.
    """
    # Validate inputs
    if page < 1 or items_per_page < 1:
        raise BadRequestError("`page` and `items_per_page` must be positive integers")

    # Count total items
    count_query = select(func.count()).select_from(query.subquery())
    total_count = await db.scalar(count_query)

    # Calculate total pages and validate if page is out of range
    total_pages = (total_count + items_per_page - 1) // items_per_page
    if page > total_pages:
        return [], Pagination(
            total_pages=total_pages,
            current_page=page,
            items_per_page=items_per_page,
        )

    offset = (page - 1) * items_per_page

    # Fetch items
    result = await db.execute(query.offset(offset).limit(items_per_page))
    items = result.all()

    if items and isinstance(items[0], Row) and len(items[0]) == 1:
        items = [item[0] for item in items]

    # Create pagination object
    pagination = Pagination(
        total_pages=total_pages,
        current_page=page,
        items_per_page=items_per_page,
    )

    # Return items and pagination
    return items, pagination


def make_naive(dt: datetime) -> datetime:
    """
    Make a timezone-aware datetime naive by converting to UTC and removing tzinfo.
    If already naive, return as-is.
    """
    if dt.tzinfo is not None:
        dt = dt.astimezone(timezone.utc)
        return dt.replace(tzinfo=None)
    return dt


def now_utc() -> datetime:
    """Get current UTC datetime with timezone."""
    return datetime.now(timezone.utc)

================
File: queries/datasets.py
================
from typing import List, Optional
from uuid import UUID

from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.dataset import Dataset


async def get_dataset_by_name(db: AsyncSession, user_id: UUID, name: str) -> Optional[Dataset]:
    """Get a dataset by name for a specific user."""
    result = await db.execute(
        select(Dataset).where(Dataset.user_id == user_id, Dataset.name == name)
    )
    return result.scalar_one_or_none()


async def list_datasets(db: AsyncSession, user_id: UUID, offset: int, limit: int) -> List[Dataset]:
    """List datasets for a specific user with pagination."""
    result = await db.execute(
        select(Dataset)
        .where(Dataset.user_id == user_id)
        .order_by(Dataset.created_at.desc())
        .offset(offset)
        .limit(limit)
    )
    return result.scalars().all()


async def count_datasets(db: AsyncSession, user_id: UUID) -> int:
    """Count total datasets for a specific user."""
    result = await db.execute(
        select(func.count()).select_from(Dataset).where(Dataset.user_id == user_id)
    )
    return result.scalar_one()

================
File: queries/fine_tuned_models.py
================
from typing import Dict, Any, Optional, Tuple, List
from uuid import UUID

from sqlalchemy import select, and_, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.fine_tuned_model import FineTunedModel
from app.models.fine_tuning_job import FineTuningJob


async def get_existing_model(
        db: AsyncSession,
        job_id: UUID,
        user_id: UUID
) -> Optional[FineTunedModel]:
    """Get existing model for a job if it exists."""
    result = await db.execute(
        select(FineTunedModel)
        .where(
            FineTunedModel.user_id == user_id,
            FineTunedModel.fine_tuning_job_id == job_id,
        )
        .order_by(FineTunedModel.created_at.desc())
    )
    return result.scalar_one_or_none()


async def create_model(
        db: AsyncSession,
        job_id: UUID,
        user_id: UUID,
        name: str,
        artifacts: Dict[str, Any]
) -> FineTunedModel:
    """Create a new fine-tuned model."""
    model = FineTunedModel(
        user_id=user_id,
        fine_tuning_job_id=job_id,
        name=name,
        artifacts=artifacts
    )
    db.add(model)
    return model


async def get_model_by_name(
        db: AsyncSession,
        user_id: UUID,
        model_name: str
) -> Optional[Tuple[FineTunedModel, str]]:
    """Get a fine-tuned model by name with its job name."""
    result = await db.execute(
        select(FineTunedModel, FineTuningJob.name.label('job_name'))
        .join(FineTuningJob, FineTunedModel.fine_tuning_job_id == FineTuningJob.id)
        .where(
            and_(
                FineTunedModel.user_id == user_id,
                FineTunedModel.name == model_name
            )
        )
    )
    return result.first()


async def list_models(
        db: AsyncSession,
        user_id: UUID,
        offset: int,
        limit: int
) -> List[Tuple[FineTunedModel, str]]:
    """List fine-tuned models with job names."""
    result = await db.execute(
        select(FineTunedModel, FineTuningJob.name.label('job_name'))
        .join(FineTuningJob, FineTunedModel.fine_tuning_job_id == FineTuningJob.id)
        .where(FineTunedModel.user_id == user_id)
        .order_by(FineTunedModel.created_at.desc())
        .offset(offset)
        .limit(limit)
    )
    return result.all()


async def count_models(db: AsyncSession, user_id: UUID) -> int:
    """Count total number of fine-tuned models for a user."""
    result = await db.execute(
        select(func.count())
        .select_from(FineTunedModel)
        .where(FineTunedModel.user_id == user_id)
    )
    return result.scalar_one()

================
File: queries/fine_tuning.py
================
from datetime import datetime, timedelta
from typing import List, Optional, Tuple
from uuid import UUID

from sqlalchemy import select, and_, or_, func
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from app.core.constants import FineTuningJobStatus
from app.models.base_model import BaseModel
from app.models.dataset import Dataset
from app.models.fine_tuning_job import FineTuningJob
from app.models.fine_tuning_job_detail import FineTuningJobDetail
from app.queries.common import make_naive, now_utc


async def get_job_with_details(
        db: AsyncSession,
        user_id: UUID,
        job_name: str
) -> Optional[Tuple[FineTuningJob, FineTuningJobDetail, str, str]]:
    """Get a fine-tuning job with its details and related names."""
    result = await db.execute(
        select(FineTuningJob, FineTuningJobDetail, BaseModel.name.label('base_model_name'),
               Dataset.name.label('dataset_name'))
        .join(FineTuningJobDetail)
        .join(BaseModel, FineTuningJob.base_model_id == BaseModel.id)
        .join(Dataset, FineTuningJob.dataset_id == Dataset.id)
        .where(
            FineTuningJob.user_id == user_id,
            FineTuningJob.name == job_name
        )
    )
    return result.first()


async def get_job_with_details_full(
        db: AsyncSession,
        job_id: UUID,
        user_id: UUID
) -> Optional[Tuple[FineTuningJob, Dataset, BaseModel, FineTuningJobDetail]]:
    """Get complete job details with all related entities."""
    result = await db.execute(
        select(FineTuningJob, Dataset, BaseModel, FineTuningJobDetail)
        .join(Dataset, FineTuningJob.dataset_id == Dataset.id)
        .join(BaseModel, FineTuningJob.base_model_id == BaseModel.id)
        .join(FineTuningJobDetail)
        .where(
            FineTuningJob.user_id == user_id,
            FineTuningJob.id == job_id
        )
    )
    return result.first()


async def get_job_by_id(
        db: AsyncSession,
        job_id: UUID,
        user_id: UUID
) -> Optional[FineTuningJob]:
    """Get a fine-tuning job by ID."""
    result = await db.execute(
        select(FineTuningJob)
        .options(selectinload(FineTuningJob.details))
        .where(
            FineTuningJob.id == job_id,
            FineTuningJob.user_id == user_id
        )
    )
    return result.scalar_one_or_none()


async def list_jobs(
        db: AsyncSession,
        user_id: UUID,
        offset: int,
        limit: int,
        exclude_deleted: bool = True
) -> List[Tuple[FineTuningJob, str, str]]:
    """List fine-tuning jobs with related names."""
    query = (
        select(FineTuningJob, BaseModel.name.label('base_model_name'), Dataset.name.label('dataset_name'))
        .join(BaseModel, FineTuningJob.base_model_id == BaseModel.id)
        .join(Dataset, FineTuningJob.dataset_id == Dataset.id)
        .where(FineTuningJob.user_id == user_id)
    )
    if exclude_deleted:
        query = query.where(FineTuningJob.status != FineTuningJobStatus.DELETED)
    query = query.order_by(FineTuningJob.created_at.desc()).offset(offset).limit(limit)

    result = await db.execute(query)
    return result.all()


async def count_jobs(
        db: AsyncSession,
        user_id: UUID,
        exclude_deleted: bool = True
) -> int:
    """Count total number of fine-tuning jobs."""
    query = select(func.count()).select_from(FineTuningJob).where(FineTuningJob.user_id == user_id)
    if exclude_deleted:
        query = query.where(FineTuningJob.status != FineTuningJobStatus.DELETED)
    result = await db.execute(query)
    return result.scalar_one()


async def get_non_terminal_jobs(
        db: AsyncSession,
        statuses: List[FineTuningJobStatus],
        completed_within_minutes: Optional[int] = None
) -> List[FineTuningJob]:
    """Get all non-terminal jobs and recently completed jobs."""
    conditions = [FineTuningJob.status.in_(statuses)]

    if completed_within_minutes:
        recent_time = now_utc() - timedelta(minutes=completed_within_minutes)
        conditions.append(and_(
            FineTuningJob.status == FineTuningJobStatus.COMPLETED,
            FineTuningJob.updated_at >= make_naive(recent_time)
        ))

    result = await db.execute(
        select(FineTuningJob)
        .options(selectinload(FineTuningJob.details))
        .where(or_(*conditions))
    )
    return result.scalars().all()


async def get_jobs_for_status_update(
        db: AsyncSession,
        non_terminal_statuses: List[FineTuningJobStatus],
        recent_completed_cutoff: datetime
) -> List[FineTuningJob]:
    """
    Get jobs that need status updates.

    Args:
        db: Database session
        non_terminal_statuses: List of statuses to include
        recent_completed_cutoff: Cutoff time for recently completed jobs

    Returns:
        List of jobs with their details loaded

    Note:
        This includes both non-terminal jobs and recently completed jobs
    """
    query = (
        select(FineTuningJob)
        .options(
            selectinload(FineTuningJob.details),
            selectinload(FineTuningJob.fine_tuned_model)
        )
        .where(
            or_(
                # Get all non-terminal jobs
                FineTuningJob.status.in_(non_terminal_statuses),
                # Get recently completed jobs
                and_(
                    FineTuningJob.status == FineTuningJobStatus.COMPLETED,
                    FineTuningJob.updated_at >= make_naive(recent_completed_cutoff)
                )
            )
        )
        .order_by(FineTuningJob.updated_at.desc())
    )

    result = await db.execute(query)
    jobs = result.scalars().all()

    return jobs

================
File: queries/models.py
================
from datetime import datetime
from typing import List, Optional, Tuple
from uuid import UUID

from sqlalchemy import select, and_, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.constants import FineTunedModelStatus
from app.models.base_model import BaseModel
from app.models.fine_tuned_model import FineTunedModel
from app.models.fine_tuning_job import FineTuningJob
from app.queries.common import make_naive


async def get_base_model_by_name(db: AsyncSession, name: str) -> Optional[BaseModel]:
    """Get a base model by name."""
    result = await db.execute(
        select(BaseModel).where(BaseModel.name == name)
    )
    return result.scalar_one_or_none()


async def list_base_models(
        db: AsyncSession,
        offset: int,
        limit: int,
        exclude_dummy: bool = True
) -> List[BaseModel]:
    """List base models with pagination."""
    query = select(BaseModel).order_by(BaseModel.name.desc()).where(BaseModel.name != 'llm_dummy')
    query = query.offset(offset).limit(limit)
    result = await db.execute(query)
    return result.scalars().all()


async def count_base_models(
        db: AsyncSession,
        exclude_dummy: bool = True
) -> int:
    """Count total number of base models."""
    query = select(func.count()).select_from(BaseModel)
    if exclude_dummy:
        query = query.where(BaseModel.name != 'llm_dummy')
    result = await db.execute(query)
    return result.scalar_one()


async def get_fine_tuned_model_by_name(
        db: AsyncSession,
        user_id: UUID,
        name: str
) -> Optional[Tuple[FineTunedModel, str]]:
    """Get a fine-tuned model by name with its associated job name."""
    result = await db.execute(
        select(FineTunedModel, FineTuningJob.name.label('job_name'))
        .join(FineTuningJob, FineTunedModel.fine_tuning_job_id == FineTuningJob.id)
        .where(
            and_(
                FineTunedModel.user_id == user_id,
                FineTunedModel.name == name,
                FineTunedModel.status != FineTunedModelStatus.DELETED
            )
        )
    )
    return result.first()


async def list_fine_tuned_models(
        db: AsyncSession,
        user_id: UUID,
        offset: int,
        limit: int
) -> List[Tuple[FineTunedModel, str]]:
    """List fine-tuned models with associated job names."""
    result = await db.execute(
        select(FineTunedModel, FineTuningJob.name.label('job_name'))
        .join(FineTuningJob, FineTunedModel.fine_tuning_job_id == FineTuningJob.id)
        .where(
            and_(
                FineTunedModel.user_id == user_id,
                FineTunedModel.status != FineTunedModelStatus.DELETED
            )
        )
        .order_by(FineTunedModel.created_at.desc())
        .offset(offset)
        .limit(limit)
    )
    return result.all()


async def count_fine_tuned_models(
        db: AsyncSession,
        user_id: UUID
) -> int:
    """Count total number of fine-tuned models for a user."""
    result = await db.execute(
        select(func.count())
        .select_from(FineTunedModel)
        .where(
            and_(
                FineTunedModel.user_id == user_id,
                FineTunedModel.status != FineTunedModelStatus.DELETED
            )
        )
    )
    return result.scalar_one()


async def get_deleted_models(
        db: AsyncSession,
        cutoff_date: datetime
) -> List[FineTunedModel]:
    """
    Get recently deleted models that need weight cleanup.

    Args:
        db: Database session
        cutoff_date: Only include models deleted after this date

    Returns:
        List of deleted models with artifacts to clean up
    """
    query = (
        select(FineTunedModel)
        .where(
            and_(
                FineTunedModel.status == FineTunedModelStatus.DELETED,
                FineTunedModel.updated_at >= make_naive(cutoff_date)
            )
        )
        .order_by(FineTunedModel.updated_at.desc())
    )

    result = await db.execute(query)
    models = result.scalars().all()

    return models

================
File: queries/usage.py
================
from datetime import date
from typing import List, Optional, Tuple
from uuid import UUID

from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.fine_tuning_job import FineTuningJob
from app.models.usage import Usage


async def get_usage_records(
        db: AsyncSession,
        user_id: UUID,
        start_date: Optional[date],
        end_date: Optional[date],
        offset: int,
        limit: int
) -> List[Tuple[Usage, str]]:
    """
    Get usage records with associated job names.

    Args:
        db: Database session
        user_id: User ID
        start_date: Optional start date filter
        end_date: Optional end date filter
        offset: Pagination offset
        limit: Number of records to return

    Returns:
        List of tuples containing Usage record and job name
    """
    query = (
        select(Usage, FineTuningJob.name.label('job_name'))
        .join(FineTuningJob, Usage.fine_tuning_job_id == FineTuningJob.id)
        .where(Usage.user_id == user_id)
    )

    if start_date:
        query = query.where(func.date(Usage.created_at) >= start_date)
    if end_date:
        query = query.where(func.date(Usage.created_at) <= end_date)

    query = (
        query.order_by(Usage.created_at.desc())
        .offset(offset)
        .limit(limit)
    )

    result = await db.execute(query)
    return result.all()


async def count_usage_records(
        db: AsyncSession,
        user_id: UUID,
        start_date: Optional[date],
        end_date: Optional[date]
) -> int:
    """Count total usage records for pagination."""
    query = select(func.count()).select_from(Usage).where(Usage.user_id == user_id)

    if start_date:
        query = query.where(func.date(Usage.created_at) >= start_date)
    if end_date:
        query = query.where(func.date(Usage.created_at) <= end_date)

    result = await db.execute(query)
    return result.scalar_one()


async def get_total_cost(
        db: AsyncSession,
        user_id: UUID,
        start_date: Optional[date],
        end_date: Optional[date]
) -> float:
    """Calculate total cost for the specified period."""
    query = select(func.sum(Usage.cost)).where(Usage.user_id == user_id)

    if start_date:
        query = query.where(func.date(Usage.created_at) >= start_date)
    if end_date:
        query = query.where(func.date(Usage.created_at) <= end_date)

    result = await db.execute(query)
    total_cost = result.scalar_one_or_none()
    return float(total_cost) if total_cost else 0.0

================
File: queries/users.py
================
from typing import Optional
from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.user import User


async def get_user_by_id(db: AsyncSession, user_id: UUID) -> Optional[User]:
    """Get a user by ID."""
    return await db.get(User, user_id)


async def get_user_by_email(db: AsyncSession, email: str) -> Optional[User]:
    """Get a user by email."""
    result = await db.execute(select(User).where(User.email == email))
    return result.scalar_one_or_none()


async def get_user_by_stripe_customer_id(db: AsyncSession, stripe_customer_id: str) -> Optional[User]:
    """Get a user by Stripe customer ID."""
    result = await db.execute(select(User).where(User.stripe_customer_id == stripe_customer_id))
    return result.scalar_one_or_none()

================
File: routes/api_keys.py
================
from typing import Dict, Union, List

from fastapi import APIRouter, Depends, status
from fastapi.params import Query
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.authentication import get_current_active_user
from app.core.config_manager import config
from app.core.database import get_db
from app.core.utils import setup_logger
from app.models.user import User
from app.schemas.api_key import ApiKeyCreate, ApiKeyResponse, ApiKeyUpdate, ApiKeyWithSecretResponse
from app.schemas.common import Pagination
from app.services.api_key import (
    create_api_key,
    get_api_keys,
    get_api_key,
    update_api_key,
    revoke_api_key,
)

# Set up API router
router = APIRouter(tags=["API Keys"])

# Set up logger
logger = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


@router.post("/api-keys", response_model=ApiKeyWithSecretResponse, status_code=status.HTTP_201_CREATED)
async def create_new_api_key(
        api_key: ApiKeyCreate,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> ApiKeyWithSecretResponse:
    """Create a new API key."""
    return await create_api_key(db, current_user.id, api_key)


@router.get("/api-keys", response_model=Dict[str, Union[List[ApiKeyResponse], Pagination]])
async def list_api_keys(
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
        page: int = Query(1, ge=1),
        items_per_page: int = Query(20, ge=1, le=100),
) -> Dict[str, Union[List[ApiKeyResponse], Pagination]]:
    """List all API keys for the current user."""
    api_keys, pagination = await get_api_keys(db, current_user.id, page, items_per_page)
    return {
        "data": api_keys,
        "pagination": pagination
    }


@router.get("/api-keys/{key_name}", response_model=ApiKeyResponse)
async def get_api_key_details(
        key_name: str,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> ApiKeyResponse:
    """Get details of a specific API key."""
    return await get_api_key(db, current_user.id, key_name)


@router.patch("/api-keys/{key_name}", response_model=ApiKeyResponse)
async def update_api_key_details(
        key_name: str,
        api_key_update: ApiKeyUpdate,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> ApiKeyResponse:
    """Update a specific API key."""
    return await update_api_key(db, current_user.id, key_name, api_key_update)


@router.delete("/api-keys/{key_name}", response_model=ApiKeyResponse)
async def revoke_api_key_route(
        key_name: str,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> ApiKeyResponse:
    """Revoke a specific API key."""
    return await revoke_api_key(db, current_user.id, key_name)

================
File: routes/auth0.py
================
from authlib.integrations.starlette_client import OAuth
from fastapi import APIRouter, Depends, Response
from fastapi.responses import RedirectResponse
from sqlalchemy.ext.asyncio import AsyncSession
from starlette.requests import Request

from app.core.config_manager import config
from app.core.database import get_db
from app.core.exceptions import ServerError
from app.core.utils import setup_logger
from app.services.auth0 import Auth0Service

router = APIRouter(tags=["Auth0"])
logger = setup_logger(__name__)

# Initialize OAuth client
oauth = OAuth()
oauth.register(
    "auth0",
    client_id=config.auth0_client_id,
    client_secret=config.auth0_client_secret,
    client_kwargs={
        "scope": "openid profile email",
    },
    server_metadata_url=f'https://{config.auth0_domain}/.well-known/openid-configuration',
)

# Initialize Auth0 service
auth0_service = Auth0Service(oauth)


@router.get("/auth0/login")
async def login(request: Request) -> RedirectResponse:
    """Initiate Auth0 login process."""
    try:
        login_url = await auth0_service.get_login_url(request)
        return RedirectResponse(url=login_url)
    except Exception as e:
        logger.error(f"Login failed: {str(e)}")
        raise ServerError("Failed to initiate login process", logger=logger)


@router.get("/auth0/callback")
async def auth0_callback(
        request: Request,
        db: AsyncSession = Depends(get_db)
) -> RedirectResponse:
    """Handle Auth0 callback after successful authentication."""
    try:
        # Process callback and get user
        session_data, _ = await auth0_service.handle_callback(request, db)

        # Store user information in session
        request.session['user'] = session_data

        # Redirect to appropriate URL
        return RedirectResponse(
            url=config.ui_url if not config.use_api_ui else request.base_url
        )
    except Exception as e:
        logger.error(f"Callback failed: {str(e)}")
        return RedirectResponse(url=request.url_for("login"))


@router.get("/auth0/logout")
async def logout(request: Request, response: Response) -> RedirectResponse:
    """Log out user from application and Auth0."""
    # Clear session
    request.session.pop('user', None)
    response.delete_cookie("session")

    # Get logout URL and redirect
    logout_url = auth0_service.get_logout_url(request)
    logger.info(f"Logging out user, redirecting to: {logout_url}")
    return RedirectResponse(url=logout_url)

================
File: routes/billing.py
================
from typing import Dict, Union, List

from fastapi import APIRouter, Depends, Query
from fastapi.responses import RedirectResponse
from sqlalchemy.ext.asyncio import AsyncSession
from starlette.requests import Request

from app.core.authentication import get_current_active_user, admin_required
from app.core.database import get_db
from app.core.utils import setup_logger
from app.models.user import User
from app.schemas.billing import CreditDeductRequest, CreditHistoryResponse, CreditAddRequest
from app.schemas.common import Pagination
from app.services.billing import (
    add_stripe_credits,
    add_manual_credits,
    deduct_credits,
    get_credit_history,
    handle_stripe_webhook, get_stripe_billing_portal_url,
)

router = APIRouter(tags=["Billing"])
logger = setup_logger(__name__)


@router.get("/billing/credit-history", response_model=Dict[str, Union[List[CreditHistoryResponse], Pagination]])
async def get_credit_history_route(
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
        start_date: str = Query(..., description="Start date (YYYY-MM-DD)"),
        end_date: str = Query(..., description="End date (YYYY-MM-DD)"),
        page: int = Query(1, ge=1),
        items_per_page: int = Query(20, ge=1, le=100),
) -> Dict[str, Union[List[CreditHistoryResponse], Pagination]]:
    """Get credit history for the current user."""
    credits, pagination = await get_credit_history(
        db, current_user.id, start_date, end_date, page, items_per_page
    )
    return {"data": credits, "pagination": pagination}


@router.get("/billing/stripe-credits-add")
async def stripe_credits_add_route(
        request: Request,
        amount_dollars: int = Query(..., description="Amount to add in dollars"),
        current_user: User = Depends(get_current_active_user),
):
    """Redirect to Stripe for adding credits."""
    checkout_url = await add_stripe_credits(current_user, amount_dollars, str(request.base_url))
    return RedirectResponse(url=checkout_url, status_code=302)


@router.post("/billing/stripe-success-callback")
async def stripe_webhook_handler(
        request: Request,
        db: AsyncSession = Depends(get_db)
):
    """Handle Stripe webhook callbacks."""
    return await handle_stripe_webhook(request, db)


@router.get("/billing/stripe-payment-method-add")
async def stripe_payment_method_add(
        request: Request,
        current_user: User = Depends(get_current_active_user),
):
    """
    Redirect to Stripe for adding a payment method.
    """
    billing_portal_session_url = await get_stripe_billing_portal_url(current_user, str(request.base_url))
    return RedirectResponse(url=billing_portal_session_url, status_code=302)


# Admin routes
@router.post("/billing/credits-deduct", response_model=CreditHistoryResponse)
async def deduct_credits_route(
        request: CreditDeductRequest,
        _: User = Depends(admin_required),
        db: AsyncSession = Depends(get_db)
):
    """Deduct credits for a job (Internal endpoint)."""
    return await deduct_credits(request, db, retry=True)


@router.post("/billing/credits-add", response_model=CreditHistoryResponse)
async def add_credits_route(
        request: CreditAddRequest,
        _: User = Depends(admin_required),
        db: AsyncSession = Depends(get_db)
):
    """Add credits to a user's account (Admin only)."""
    return await add_manual_credits(db, request)

================
File: routes/datasets.py
================
from typing import Dict, Union, List

from fastapi import APIRouter, Depends, status, UploadFile, File
from fastapi.params import Query, Form
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.authentication import get_current_active_user
from app.core.config_manager import config
from app.core.database import get_db
from app.core.utils import setup_logger
from app.models.user import User
from app.schemas.common import Pagination
from app.schemas.dataset import DatasetCreate, DatasetResponse, DatasetUpdate
from app.services.dataset import (
    create_dataset,
    get_datasets,
    get_dataset,
    update_dataset,
    delete_dataset,
)

# Set up API router
router = APIRouter(tags=["Datasets"])

# Set up logger
logger = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


@router.post("/datasets", response_model=DatasetResponse, status_code=status.HTTP_201_CREATED)
async def upload_dataset(
        file: UploadFile = File(...),
        name: str = Form(...),
        description: str = Form(None),
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> DatasetResponse:
    """Upload a new dataset file."""
    dataset_create = DatasetCreate(
        name=name,
        description=description,
        file=file,
    )
    return await create_dataset(db, current_user.id, dataset_create)


@router.get("/datasets", response_model=Dict[str, Union[List[DatasetResponse], Pagination]])
async def list_datasets(
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
        page: int = Query(1, ge=1),
        items_per_page: int = Query(20, ge=1, le=100),
) -> Dict[str, Union[List[DatasetResponse], Pagination]]:
    """List all datasets uploaded by the user."""
    datasets, pagination = await get_datasets(db, current_user.id, page, items_per_page)
    return {
        "data": datasets,
        "pagination": pagination
    }


@router.get("/datasets/{dataset_name}", response_model=DatasetResponse)
async def get_dataset_info(
        dataset_name: str,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> DatasetResponse:
    """Get information about a specific dataset."""
    return await get_dataset(db, current_user.id, dataset_name)


@router.patch("/datasets/{dataset_name}", response_model=DatasetResponse)
async def update_dataset_info(
        dataset_name: str,
        dataset_update: DatasetUpdate,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> DatasetResponse:
    """Update a specific dataset's information."""
    return await update_dataset(db, current_user.id, dataset_name, dataset_update)


@router.delete("/datasets/{dataset_name}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_dataset_file(
        dataset_name: str,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> None:
    """Delete a specific dataset file."""
    await delete_dataset(db, current_user.id, dataset_name)

================
File: routes/fine_tuning.py
================
from typing import Dict, Union, List

from fastapi import APIRouter, Depends, status
from fastapi.params import Query
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.authentication import get_current_active_user
from app.core.constants import FineTuningJobType
from app.core.database import get_db
from app.core.exceptions import BadRequestError
from app.core.utils import setup_logger
from app.models.user import User
from app.schemas.common import Pagination
from app.schemas.fine_tuning import (
    FineTuningJobCreate,
    FineTuningJobResponse,
    FineTuningJobDetailResponse
)
from app.services.fine_tuning import (
    create_fine_tuning_job,
    get_fine_tuning_jobs,
    get_fine_tuning_job,
    cancel_fine_tuning_job,
    delete_fine_tuning_job
)

router = APIRouter(tags=["Fine-tuning Jobs"])
logger = setup_logger(__name__)


@router.post("/fine-tuning", response_model=FineTuningJobDetailResponse, status_code=status.HTTP_201_CREATED)
async def create_new_fine_tuning_job(
        job: FineTuningJobCreate,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> FineTuningJobDetailResponse:
    """Create a new fine-tuning job."""
    if job.type == FineTuningJobType.FULL:
        raise BadRequestError('FULL fine-tuning is not supported yet, but we are working on it')
    return await create_fine_tuning_job(db, current_user, job)


@router.get("/fine-tuning", response_model=Dict[str, Union[List[FineTuningJobResponse], Pagination]])
async def list_fine_tuning_jobs(
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
        page: int = Query(1, ge=1),
        items_per_page: int = Query(20, ge=1, le=100),
) -> Dict[str, Union[List[FineTuningJobResponse], Pagination]]:
    """List all fine-tuning jobs for the current user."""
    jobs, pagination = await get_fine_tuning_jobs(db, current_user.id, page, items_per_page)
    return {
        "data": jobs,
        "pagination": pagination
    }


@router.get("/fine-tuning/{job_name}", response_model=FineTuningJobDetailResponse)
async def get_fine_tuning_job_details(
        job_name: str,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> FineTuningJobDetailResponse:
    """Get details of a specific fine-tuning job."""
    return await get_fine_tuning_job(db, current_user.id, job_name)


@router.post("/fine-tuning/{job_name}/cancel", response_model=FineTuningJobDetailResponse)
async def cancel_fine_tuning_job_route(
        job_name: str,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> FineTuningJobDetailResponse:
    """Cancel a fine-tuning job."""
    return await cancel_fine_tuning_job(db, current_user.id, job_name)


@router.delete("/fine-tuning/{job_name}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_fine_tuning_job_route(
        job_name: str,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> None:
    """Delete a fine-tuning job."""
    await delete_fine_tuning_job(db, current_user.id, job_name)

================
File: routes/models.py
================
from typing import Dict, Union, List

from fastapi import APIRouter, Depends
from fastapi.params import Query
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.authentication import get_current_active_user
from app.core.config_manager import config
from app.core.database import get_db
from app.core.utils import setup_logger
from app.models.user import User
from app.schemas.common import Pagination
from app.schemas.model import BaseModelResponse, FineTunedModelResponse
from app.services.fine_tuned_model import (
    get_fine_tuned_models,
    get_fine_tuned_model
)
from app.services.model import (
    get_base_models,
    get_base_model,
)

router = APIRouter(tags=["Models"])
logger = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


@router.get("/models/base", response_model=Dict[str, Union[List[BaseModelResponse], Pagination]])
async def list_base_models(
        db: AsyncSession = Depends(get_db),
        page: int = Query(1, ge=1, description="Page number"),
        items_per_page: int = Query(20, ge=1, le=100, description="Number of items per page")
) -> Dict[str, Union[List[BaseModelResponse], Pagination]]:
    """List all available base LLM models."""
    models, pagination = await get_base_models(db, page, items_per_page)
    return {
        "data": models,
        "pagination": pagination
    }


@router.get("/models/base/{model_name}", response_model=BaseModelResponse)
async def get_base_model_details(
        model_name: str,
        db: AsyncSession = Depends(get_db),
) -> BaseModelResponse:
    """Get detailed information about a specific base model."""
    return await get_base_model(db, model_name)


@router.get("/models/fine-tuned", response_model=Dict[str, Union[List[FineTunedModelResponse], Pagination]])
async def list_fine_tuned_models(
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
        page: int = Query(1, ge=1, description="Page number"),
        items_per_page: int = Query(20, ge=1, le=100, description="Number of items per page")
) -> Dict[str, Union[List[FineTunedModelResponse], Pagination]]:
    """List all fine-tuned models for the current user."""
    models, pagination = await get_fine_tuned_models(db, current_user.id, page, items_per_page)
    return {
        "data": models,
        "pagination": pagination
    }


@router.get("/models/fine-tuned/{model_name}", response_model=FineTunedModelResponse)
async def get_fine_tuned_model_details(
        model_name: str,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> FineTunedModelResponse:
    """Get detailed information about a specific fine-tuned model."""
    return await get_fine_tuned_model(db, current_user.id, model_name)

================
File: routes/usage.py
================
from typing import Dict, Union, List

from fastapi import APIRouter, Depends, Query
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.authentication import get_current_active_user
from app.core.database import get_db
from app.core.utils import setup_logger
from app.models.user import User
from app.schemas.common import Pagination
from app.schemas.usage import TotalCostResponse, UsageRecordResponse
from app.services.usage import get_total_cost, get_usage_records

router = APIRouter(tags=["Usage"])
logger = setup_logger(__name__)


@router.get("/usage/total-cost", response_model=TotalCostResponse)
async def get_total_cost_route(
        start_date: str = Query(..., description="Start date (YYYY-MM-DD)"),
        end_date: str = Query(..., description="End date (YYYY-MM-DD)"),
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> TotalCostResponse:
    """Get total cost for a given period."""
    return await get_total_cost(db, current_user.id, start_date, end_date)


@router.get("/usage/records", response_model=Dict[str, Union[List[UsageRecordResponse], Pagination]])
async def list_usage_records(
        start_date: str = Query(..., description="Start date (YYYY-MM-DD)"),
        end_date: str = Query(..., description="End date (YYYY-MM-DD)"),
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
        page: int = Query(1, ge=1),
        items_per_page: int = Query(20, ge=1, le=100),
) -> Dict[str, Union[List[UsageRecordResponse], Pagination]]:
    """Get a list of usage records for a given period."""
    records, pagination = await get_usage_records(
        db, current_user.id, start_date, end_date, page, items_per_page
    )
    return {
        "data": records,
        "pagination": pagination
    }

================
File: routes/users.py
================
from uuid import UUID

from fastapi import APIRouter, Depends, status
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.authentication import get_current_active_user, admin_required
from app.core.config_manager import config
from app.core.database import get_db
from app.core.utils import setup_logger
from app.models.user import User
from app.schemas.user import UserUpdate, UserResponse
from app.services.user import update_user, deactivate_user

# Set up API router
router = APIRouter(tags=["Users"])

# Set up logger
logger = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


@router.get("/users/me", response_model=UserResponse)
async def get_current_user_info(
        current_user: User = Depends(get_current_active_user)
) -> UserResponse:
    """Get current user's information."""
    logger.info(f"Retrieved information for user: {current_user.id}")
    return UserResponse.from_orm(current_user)


@router.patch("/users/me", response_model=UserResponse)
async def update_current_user(
        user_update: UserUpdate,
        current_user: User = Depends(get_current_active_user),
        db: AsyncSession = Depends(get_db),
) -> UserResponse:
    """Update current user's information."""
    return await update_user(db, current_user.id, user_update)


@router.delete("/users/{user_id}", status_code=status.HTTP_204_NO_CONTENT)
async def deactivate_user_route(
        user_id: UUID,
        _: User = Depends(admin_required),  # Admin check
        db: AsyncSession = Depends(get_db)
) -> None:
    """Deactivate a user's account (admin only)."""
    await deactivate_user(db, user_id)

================
File: schemas/api_key.py
================
from datetime import datetime, timezone
from uuid import UUID

from pydantic import BaseModel, Field, ConfigDict, field_validator

from app.core.constants import ApiKeyStatus
from app.core.exceptions import BadRequestError
from app.schemas.common import NameField, DateTime


def _expiration_must_be_future(v: datetime) -> datetime:
    if v.astimezone(timezone.utc) <= datetime.utcnow().astimezone(timezone.utc):
        raise BadRequestError('Expiration date must be in the future')
    return v


class ApiKeyCreate(BaseModel):
    """
    Schema for creating a new API key.
    """
    name: str = NameField(..., description="The name of the API key")
    expires_at: DateTime = Field(..., description="The expiration date and time of the API key")

    @field_validator('expires_at')
    def expiration_must_be_future(cls, v: datetime) -> datetime:
        return _expiration_must_be_future(v)


class ApiKeyUpdate(BaseModel):
    """
    Schema for updating an existing API key.
    """
    name: str | None = NameField(None, description="The new name for the API key")
    expires_at: datetime | None = Field(None, description="The new expiration date and time for the API key")

    @field_validator('expires_at')
    def expiration_must_be_future(cls, v: datetime) -> datetime:
        return _expiration_must_be_future(v)


class ApiKeyResponse(BaseModel):
    """
    Schema for API key response data.
    """
    id: UUID = Field(..., description="The unique identifier of the API key")
    created_at: DateTime = Field(..., description="The creation date and time of the API key")
    expires_at: DateTime = Field(..., description="The expiration date and time of the API key")
    status: ApiKeyStatus = Field(..., description="The current status of the API key")
    name: str = Field(..., description="The name of the API key")
    prefix: str = Field(..., description="The prefix of the API key (first few characters)")
    model_config = ConfigDict(from_attributes=True)


class ApiKeyWithSecretResponse(ApiKeyResponse):
    """
    Schema for API key response data including the secret key.
    This schema should only be used when creating a new API key to return the secret key to the user.
    """
    secret: str = Field(..., description="The full API key secret")
    model_config = ConfigDict(from_attributes=True)

================
File: schemas/billing.py
================
from uuid import UUID

from pydantic import BaseModel, Field, ConfigDict

from app.core.constants import UsageUnit, ServiceName, BillingTransactionType
from app.schemas.common import DateTime


class CreditDeductRequest(BaseModel):
    """
    Schema for committing credits for a job.
    """
    user_id: UUID = Field(..., description="The ID of the user")
    usage_amount: int = Field(..., description="The amount of usage")
    usage_unit: UsageUnit = Field(..., description="The unit of usage")
    service_name: ServiceName = Field(..., description="The name of the service")
    fine_tuning_job_id: UUID = Field(..., description="The ID of the job")


class CreditAddRequest(BaseModel):
    """
    Schema for adding credits to a user's account.
    """
    user_id: UUID = Field(..., description="The ID of the user")
    amount: float = Field(..., description="The amount of credits to add")
    transaction_id: str = Field(..., description="The transaction ID")


class CreditHistoryResponse(BaseModel):
    """
    Schema for credit history response data.
    """
    id: UUID = Field(..., description="The unique identifier for the credit record")
    created_at: DateTime = Field(..., description="The timestamp when the credit record was created")
    credits: float = Field(..., description="The amount of credits added or deducted")
    transaction_id: str = Field(..., description="The transaction ID")
    transaction_type: BillingTransactionType = Field(..., description="The type of transaction")
    model_config = ConfigDict(from_attributes=True)

================
File: schemas/common.py
================
from datetime import datetime
from functools import partial
from typing import Annotated

from pydantic import BaseModel, Field, PlainSerializer


class Pagination(BaseModel):
    """
    Schema for pagination information. Used to provide pagination details in API responses.
    """
    total_pages: int
    current_page: int
    items_per_page: int


NameField = partial(Field,
                    min_length=1, max_length=255,
                    pattern="^[a-z0-9-]+$")

DateTime = Annotated[
    datetime,
    PlainSerializer(lambda _datetime: _datetime.strftime("%Y-%m-%dT%H:%M:%SZ"), return_type=str),
]

================
File: schemas/dataset.py
================
from uuid import UUID

from fastapi import UploadFile
from pydantic import BaseModel, Field, ConfigDict, field_validator

from app.core.constants import DatasetStatus
from app.core.exceptions import BadRequestError
from app.schemas.common import NameField, DateTime


class DatasetCreate(BaseModel):
    """
    Schema for creating a new dataset.
    """
    name: str = NameField(..., description="The name of the dataset")
    description: str | None = Field(None, max_length=1000, description="A description of the dataset")
    file: UploadFile = Field(..., description="The uploaded dataset file")

    @field_validator('file')
    def validate_file_size(cls, v):
        max_size = 100 * 1024 * 1024  # 100 MB
        if v.size > max_size:
            raise BadRequestError(f"File size must not exceed {max_size} bytes, got {v.size} bytes")
        return v


class DatasetUpdate(BaseModel):
    """
    Schema for updating an existing dataset.
    """
    name: str | None = NameField(None, description="The new name for the dataset")
    description: str | None = Field(None, max_length=1000, description="The new description for the dataset")


class DatasetResponse(BaseModel):
    """
    Schema for dataset response data.
    """
    id: UUID = Field(..., description="The unique identifier of the dataset")
    created_at: DateTime = Field(..., description="The timestamp when the dataset was created")
    updated_at: DateTime = Field(..., description="The timestamp when the dataset was last updated")
    status: DatasetStatus = Field(..., description="The current status of the dataset")
    name: str = Field(..., description="The name of the dataset")
    description: str | None = Field(None, description="The description of the dataset, if any")
    file_name: str = Field(..., description="The name of the stored dataset file")
    file_size: int = Field(..., description="The size of the dataset file in bytes")
    errors: dict | None = Field(None, description="Any errors encountered during dataset processing, if any")
    model_config = ConfigDict(from_attributes=True)

================
File: schemas/fine_tuning.py
================
from typing import Dict, Any
from uuid import UUID

from pydantic import BaseModel, Field, ConfigDict

from app.core.constants import FineTuningJobStatus, FineTuningJobType, ComputeProvider
from app.schemas.common import NameField, DateTime


class FineTuningJobParameters(BaseModel):
    """Model for fine-tuning job parameters."""
    batch_size: int = Field(default=2, gt=0, le=8)
    shuffle: bool = Field(default=True)
    num_epochs: int = Field(default=1, gt=0, le=10)
    lr: float = Field(default=3e-4, gt=0, le=1)
    seed: int | None = Field(None, ge=0)


class FineTuningJobCreate(BaseModel):
    """
    Schema for creating a new fine-tuning job.
    """
    base_model_name: str = Field(..., description="The name of the base model to use for fine-tuning")
    dataset_name: str = Field(..., description="The name of the dataset to use for fine-tuning")
    name: str = NameField(..., description="The name of the fine-tuning job")
    type: FineTuningJobType = Field(..., description="The type of fine-tuning job to run")
    provider: ComputeProvider = Field(ComputeProvider.GCP, description="The compute provider to use for fine-tuning")
    parameters: Dict[str, Any] = Field(..., description="The parameters for the fine-tuning job")


class FineTuningJobResponse(BaseModel):
    """
    Schema for fine-tuning job response data.
    """
    id: UUID = Field(..., description="The unique identifier of the fine-tuning job")
    created_at: DateTime = Field(..., description="The creation date and time of the fine-tuning job")
    updated_at: DateTime = Field(..., description="The last update date and time of the fine-tuning job")
    base_model_name: str = Field(..., description="The name of the base model used for fine-tuning")
    dataset_name: str = Field(..., description="The name of the dataset used for fine-tuning")
    status: FineTuningJobStatus = Field(..., description="The current status of the fine-tuning job")
    name: str = Field(..., description="The name of the fine-tuning job")
    type: FineTuningJobType = Field(..., description="The type of fine-tuning job")
    provider: ComputeProvider = Field(..., description="The compute provider used for fine-tuning")
    current_step: int | None = Field(None, description="The current step of the fine-tuning process")
    total_steps: int | None = Field(None, description="The total number of steps in the fine-tuning process")
    current_epoch: int | None = Field(None, description="The current epoch of the fine-tuning process")
    total_epochs: int | None = Field(None, description="The total number of epochs in the fine-tuning process")
    num_tokens: int | None = Field(None, description="The number of tokens processed in the fine-tuning job")
    model_config = ConfigDict(from_attributes=True)


class FineTuningJobDetailResponse(FineTuningJobResponse):
    """
    Schema for detailed fine-tuning job response data, including parameters and metrics.
    """
    parameters: Dict[str, Any] = Field(..., description="The parameters used for the fine-tuning job")
    metrics: Dict[str, Any] | None = Field(None, description="The metrics collected during the fine-tuning process")
    timestamps: Dict[str, Any] | None = Field(None,
                                              description="The timestamps recorded during the fine-tuning process")
    model_config = ConfigDict(from_attributes=True)

================
File: schemas/model.py
================
from typing import Dict, Any
from uuid import UUID

from pydantic import BaseModel, Field, ConfigDict

from app.core.constants import BaseModelStatus, FineTunedModelStatus
from app.schemas.common import DateTime


class BaseModelResponse(BaseModel):
    """
    Schema for base model response data.
    """
    id: UUID = Field(..., description="The unique identifier of the base model")
    description: str | None = Field(None, description="A description of the base model")
    hf_url: str = Field(..., description="The Hugging Face URL for the model")
    status: BaseModelStatus = Field(..., description="The current status of the base model")
    name: str = Field(..., description="The name of the base model")
    meta: Dict[str, Any] | None = Field(None, description="Additional metadata about the base model")
    model_config = ConfigDict(from_attributes=True)


class FineTunedModelResponse(BaseModel):
    """
    Schema for fine-tuned model response data.
    """
    id: UUID = Field(..., description="The unique identifier of the fine-tuned model")
    created_at: DateTime = Field(..., description="The timestamp when the fine-tuned model was created")
    updated_at: DateTime = Field(..., description="The timestamp when the fine-tuned model was last updated")
    fine_tuning_job_name: str = Field(..., description="The name of the associated fine-tuning job")
    status: FineTunedModelStatus = Field(..., description="The current status of the fine-tuned model")
    name: str = Field(..., description="The name of the fine-tuned model")
    artifacts: Dict[str, Any] | None = Field(None,
                                             description="Additional artifacts associated with the fine-tuned model")
    model_config = ConfigDict(from_attributes=True)

================
File: schemas/usage.py
================
from datetime import date
from uuid import UUID

from pydantic import BaseModel, Field, ConfigDict

from app.core.constants import UsageUnit, ServiceName
from app.schemas.common import DateTime


class UsageRecordResponse(BaseModel):
    """
    Schema for usage record response data.
    """
    id: UUID = Field(..., description="The unique identifier of the usage record")
    created_at: DateTime = Field(..., description="The timestamp when the usage record was created")
    service_name: ServiceName = Field(..., description="The name of the service used")
    usage_amount: float = Field(..., description="The amount of usage for the service")
    usage_unit: UsageUnit = Field(..., description="The unit of usage for the service")
    cost: float = Field(..., description="The cost associated with the usage")
    fine_tuning_job_name: str = Field(..., description="The name of the associated fine-tuning job")
    model_config = ConfigDict(from_attributes=True)


class TotalCostResponse(BaseModel):
    """
    Schema for total cost response data.
    """
    start_date: date = Field(..., description="The start date of the period for which the cost is calculated")
    end_date: date = Field(..., description="The end date of the period for which the cost is calculated")
    total_cost: float = Field(..., description="The total cost for the specified period")
    model_config = ConfigDict(from_attributes=True)

================
File: schemas/user.py
================
from uuid import UUID

from pydantic import BaseModel, EmailStr, Field, ConfigDict

from app.core.constants import UserStatus
from app.schemas.common import DateTime


class UserUpdate(BaseModel):
    """
    Schema for updating user information.
    """
    name: str | None = Field(None, min_length=1, max_length=255, description="The updated name of the user")


class UserResponse(BaseModel):
    """
    Schema for user response data.
    """
    id: UUID = Field(..., description="The unique identifier for the user")
    created_at: DateTime = Field(..., description="The timestamp when the user was created")
    updated_at: DateTime = Field(..., description="The timestamp when the user was last updated")
    status: UserStatus = Field(..., description="The current status of the user")
    name: str = Field(..., description="The name of the user")
    email: EmailStr = Field(..., description="The email address of the user")
    credits_balance: float = Field(..., description="The current credit balance of the user")
    model_config = ConfigDict(from_attributes=True)

================
File: services/api_key.py
================
from datetime import timezone
from uuid import UUID

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config_manager import config
from app.core.constants import ApiKeyStatus
from app.core.cryptography import generate_api_key
from app.core.exceptions import (
    ApiKeyAlreadyExistsError,
    ApiKeyNotFoundError,
)
from app.core.utils import setup_logger
from app.models.api_key import ApiKey
from app.queries import api_keys as api_key_queries
from app.schemas.api_key import ApiKeyCreate, ApiKeyUpdate, ApiKeyResponse, ApiKeyWithSecretResponse
from app.schemas.common import Pagination

# Set up logger
logger = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


async def create_api_key(db: AsyncSession, user_id: UUID, api_key: ApiKeyCreate) -> ApiKeyWithSecretResponse:
    """Create a new API key for a user."""
    # Check if an API key with the same name already exists for this user
    existing_key = await api_key_queries.get_api_key_by_name(db, user_id, api_key.name)
    if existing_key:
        raise ApiKeyAlreadyExistsError(f"An API key with the name {api_key.name} "
                                       f"already exists for user {user_id}", logger)

    # Generate a new API key
    key, key_hash = generate_api_key()
    key_prefix = key[:8]

    # Ensure the expires_at field is stored as timezone-naive UTC datetime
    expires_at = api_key.expires_at.astimezone(timezone.utc)
    expires_at = expires_at.replace(tzinfo=None)

    # Store the API key in the database
    db_api_key = ApiKey(
        user_id=user_id,
        name=api_key.name,
        expires_at=expires_at,
        prefix=key_prefix,
        key_hash=key_hash,
        status=ApiKeyStatus.ACTIVE,
    )
    db.add(db_api_key)
    await db.commit()
    await db.refresh(db_api_key)

    # Restore the timezone to the expires_at field
    db_api_key.expires_at = expires_at.replace(tzinfo=timezone.utc)

    logger.info(f"Created new API key for user: {user_id}, prefix: {key_prefix}")
    return ApiKeyWithSecretResponse(
        **ApiKeyResponse.from_orm(db_api_key).dict(),
        secret=key
    )


async def get_api_keys(
        db: AsyncSession,
        user_id: UUID,
        page: int = 1,
        items_per_page: int = 20
) -> tuple[list[ApiKeyResponse], Pagination]:
    """Get all API keys for a user with pagination."""
    offset = (page - 1) * items_per_page

    # Get total count and paginated results
    total_count = await api_key_queries.count_api_keys(db, user_id)
    api_keys = await api_key_queries.list_api_keys(db, user_id, offset, items_per_page)

    # Calculate pagination
    total_pages = (total_count + items_per_page - 1) // items_per_page
    pagination = Pagination(
        total_pages=total_pages,
        current_page=page,
        items_per_page=items_per_page,
    )

    # Create response objects
    api_key_responses = [ApiKeyResponse.from_orm(key) for key in api_keys]

    logger.info(f"Retrieved API keys for user: {user_id}, page: {page}")
    return api_key_responses, pagination


async def get_api_key(db: AsyncSession, user_id: UUID, key_name: str) -> ApiKeyResponse:
    """Get a specific API key."""
    api_key = await api_key_queries.get_api_key_by_name(db, user_id, key_name)
    if not api_key:
        raise ApiKeyNotFoundError(f"API key not found: {key_name} for user: {user_id}", logger)

    logger.info(f"Retrieved API key: {key_name} for user: {user_id}")
    return ApiKeyResponse.from_orm(api_key)


async def update_api_key(db: AsyncSession, user_id: UUID, key_name: str,
                         api_key_update: ApiKeyUpdate) -> ApiKeyResponse:
    """Update an API key."""
    db_api_key = await api_key_queries.get_api_key_by_name(db, user_id, key_name)
    if not db_api_key:
        raise ApiKeyNotFoundError(f"API key not found: {key_name} for user: {user_id}", logger)

    new_db_api_key = await api_key_queries.get_api_key_by_name(db, user_id, api_key_update.name)
    if new_db_api_key:
        raise ApiKeyAlreadyExistsError(f"An API key with the name '{api_key_update.name}' already "
                                       f"exists for user {user_id}", logger)

    # Ensure the expires_at field is stored as timezone-naive UTC datetime
    expires_at = api_key_update.expires_at
    if expires_at:
        expires_at = expires_at.astimezone(timezone.utc)
        expires_at = expires_at.replace(tzinfo=None)
        api_key_update.expires_at = expires_at

    # Update the API key fields
    update_data = api_key_update.dict(exclude_unset=True)
    for field, value in update_data.items():
        setattr(db_api_key, field, value)

    await db.commit()
    await db.refresh(db_api_key)

    # Restore the timezone to the expires_at field
    db_api_key.expires_at = db_api_key.expires_at.replace(tzinfo=timezone.utc)

    logger.info(f"Updated API key: {key_name} for user: {user_id}")
    return ApiKeyResponse.from_orm(db_api_key)


async def revoke_api_key(db: AsyncSession, user_id: UUID, key_name: str) -> ApiKeyResponse:
    """Revoke an API key."""
    db_api_key = await api_key_queries.get_api_key_by_name(db, user_id, key_name)
    if not db_api_key:
        raise ApiKeyNotFoundError(f"API key not found: {key_name} for user: {user_id}", logger)

    db_api_key.status = ApiKeyStatus.REVOKED
    await db.commit()
    await db.refresh(db_api_key)

    logger.info(f"Revoked API key: {key_name} for user: {user_id}")
    return ApiKeyResponse.from_orm(db_api_key)

================
File: services/auth0.py
================
from typing import Dict, Tuple
from urllib.parse import quote_plus, urlencode

from sqlalchemy.ext.asyncio import AsyncSession
from starlette.requests import Request

from app.core.config_manager import config
from app.core.constants import BillingTransactionType
from app.core.utils import setup_logger
from app.models.user import User
from app.queries import users as user_queries
from app.services.billing import add_credits_to_user

logger = setup_logger(__name__)


class Auth0Service:
    """Service for handling Auth0 authentication."""

    def __init__(self, oauth):
        self.oauth = oauth
        self.client_id = config.auth0_client_id
        self.domain = config.auth0_domain
        self.ui_url = config.ui_url
        self.use_api_ui = config.use_api_ui
        self.new_user_credits = float(config.new_user_credits)

    async def get_login_url(self, request: Request) -> str:
        """
        Generate Auth0 login URL.

        Args:
            request: The incoming request

        Returns:
            URL to redirect user for Auth0 login
        """
        try:
            redirect_uri = request.url_for("auth0_callback")
            logger.info(f"Generated Auth0 login URL with redirect URI: {redirect_uri}")
            result = await self.oauth.auth0.authorize_redirect(request, redirect_uri)
            return result.headers["location"]
        except Exception as e:
            logger.error(f"Failed to generate Auth0 login URL: {str(e)}")
            raise

    def get_logout_url(self, request: Request) -> str:
        """
        Generate Auth0 logout URL.

        Args:
            request: The incoming request

        Returns:
            URL to redirect user for Auth0 logout
        """
        return_to = self.ui_url if not self.use_api_ui else str(request.base_url)

        return f"https://{self.domain}/v2/logout?" + urlencode(
            {
                "returnTo": return_to,
                "client_id": self.client_id,
            },
            quote_via=quote_plus,
        )

    async def handle_callback(
            self,
            request: Request,
            db: AsyncSession
    ) -> Tuple[Dict[str, str], User]:
        """
        Handle Auth0 callback and create/update user.

        Args:
            request: The incoming request
            db: Database session

        Returns:
            Tuple of session data and user object

        Raises:
            ValueError: If required user info is missing
        """
        # Get token and user info from Auth0
        token = await self.oauth.auth0.authorize_access_token(request)
        user_info = token.get('userinfo')

        if not user_info:
            logger.warning("No user info found in Auth0 token")
            raise ValueError("No user info found in Auth0 token")

        # Extract user info
        email = user_info['email']
        name = user_info['name']
        auth0_user_id = user_info['sub']
        email_verified = user_info['email_verified']

        # Get or create user
        user = await self._get_or_create_user(
            db, name, email, auth0_user_id, email_verified
        )

        # Create session data
        session_data = {
            'id': str(user.id),
            'email': user.email,
            'name': user.name
        }

        logger.info(f"Successfully authenticated user: {email}")
        return session_data, user

    async def _get_or_create_user(
            self,
            db: AsyncSession,
            name: str,
            email: str,
            auth0_user_id: str,
            email_verified: bool
    ) -> User:
        """
        Get existing user or create new one.

        Args:
            db: Database session
            name: User's name
            email: User's email
            auth0_user_id: Auth0 user ID
            email_verified: Whether email is verified

        Returns:
            User object
        """
        # Check if user exists
        user = await user_queries.get_user_by_email(db, email)

        try:
            if user:
                # Update existing user if needed
                if user.email_verified != email_verified:
                    user.email_verified = email_verified
                    await db.commit()
                    logger.info(f"Updated email verification status for user: {email}")
            else:
                # Create new user
                user = User(
                    email=email,
                    name=name,
                    auth0_user_id=auth0_user_id,
                    email_verified=email_verified
                )
                db.add(user)
                await db.commit()

                # Add new user credits if configured
                if self.new_user_credits > 0:
                    await add_credits_to_user(
                        db,
                        user.id,
                        self.new_user_credits,
                        "NEW_USER_CREDIT",
                        BillingTransactionType.NEW_USER_CREDIT
                    )

                logger.info(f"Created new user: {email}")

            return user

        except Exception as e:
            await db.rollback()
            logger.error(f"Error in user creation/update: {str(e)}")
            raise

================
File: services/billing.py
================
import asyncio
from datetime import datetime
from uuid import UUID

import stripe
from sqlalchemy.exc import IntegrityError
from sqlalchemy.ext.asyncio import AsyncSession
from starlette.requests import Request

from app.core.config_manager import config
from app.core.constants import BillingTransactionType, UsageUnit
from app.core.exceptions import (
    BadRequestError,
    PaymentNeededError,
    ServerError,
    UserNotFoundError
)
from app.core.stripe_client import (
    create_stripe_checkout_session,
    stripe_charge_offline, create_stripe_billing_portal_session
)
from app.core.utils import setup_logger
from app.models.billing_credit import BillingCredit
from app.models.fine_tuning_job import FineTuningJob
from app.models.usage import Usage
from app.models.user import User
from app.queries import billing as billing_queries
from app.queries import users as user_queries
from app.schemas.billing import (
    CreditDeductRequest,
    CreditAddRequest,
    CreditHistoryResponse
)
from app.schemas.common import Pagination

logger = setup_logger(__name__)


async def add_stripe_credits(
        user: User,
        amount_dollars: int,
        base_url: str
) -> str:
    """Create Stripe checkout session for adding credits."""
    success_url = (
        f"{config.ui_url}{config.ui_url_settings}?stripe_success=1"
        if not config.use_api_ui else base_url
    )
    cancel_url = (
        f"{config.ui_url}{config.ui_url_settings}?stripe_error=user_cancelled"
        if not config.use_api_ui else base_url
    )

    try:
        checkout_session = create_stripe_checkout_session(
            user, amount_dollars, success_url, cancel_url
        )
        return checkout_session.url
    except Exception as e:
        raise ServerError(f"Failed to create Stripe checkout session: {str(e)}", logger)


async def get_stripe_billing_portal_url(user: User, base_url: str) -> str:
    """ Create a Stripe billing portal session """
    billing_portal_session = create_stripe_billing_portal_session(
        user,
        config.ui_url + config.ui_url_settings + "?stripe_success=2" if not config.use_api_ui else base_url,
    )
    return billing_portal_session.url


async def add_manual_credits(
        db: AsyncSession,
        request: CreditAddRequest
) -> CreditHistoryResponse:
    """Add credits manually (admin only)."""
    # Verify user exists
    user = await user_queries.get_user_by_id(db, request.user_id)
    if not user:
        raise UserNotFoundError(f"User not found: {request.user_id}", logger)

    try:
        # Add credits to user's balance
        user.credits_balance += request.amount

        # Record credit addition
        credit_record = BillingCredit(
            user_id=user.id,
            credits=request.amount,
            transaction_id=request.transaction_id,
            transaction_type=BillingTransactionType.MANUAL_ADJUSTMENT
        )
        db.add(credit_record)
        await db.commit()
        await db.refresh(credit_record)

        logger.info(f"Added {request.amount} credits to user: {user.id}")
        return CreditHistoryResponse.from_orm(credit_record)
    except IntegrityError:
        await db.rollback()
        raise BadRequestError(f"Transaction already exists: {request.transaction_id}, "
                              f"use a different transaction ID", logger)
    except Exception as e:
        await db.rollback()
        raise ServerError(f"Failed to add credits: {str(e)}", logger)


async def deduct_credits(
        request: CreditDeductRequest,
        db: AsyncSession,
        retry: bool = False
) -> CreditHistoryResponse:
    """Deduct credits for a service usage."""
    # Get job and user information
    user = await user_queries.get_user_by_id(db, request.user_id)
    if not user:
        raise BadRequestError(f"User not found: {request.user_id}")

    job_info = await billing_queries.get_job_for_credits(db, request.fine_tuning_job_id, request.user_id)
    if not job_info:
        raise BadRequestError(f"Job not found: {request.fine_tuning_job_id}")

    job, base_model_name = job_info

    # Check for existing deduction
    existing_credit = await billing_queries.get_credit_record(
        db,
        request.user_id,
        str(job.id),
        BillingTransactionType.FINE_TUNING_JOB
    )
    if existing_credit:
        return CreditHistoryResponse.from_orm(existing_credit)

    # Calculate required credits
    required_credits = await calculate_required_credits(
        request.usage_amount,
        request.usage_unit,
        base_model_name
    )

    try:
        if user.credits_balance >= required_credits:
            return await process_credit_deduction(db, user, job, required_credits, request)
        elif retry:
            return await handle_insufficient_credits(
                db, user, job, required_credits, request
            )
        else:
            raise PaymentNeededError(f"Insufficient credits: {user.credits_balance}/{required_credits}", logger)

    except Exception as e:
        await db.rollback()
        raise e


async def process_credit_deduction(
        db: AsyncSession,
        user: User,
        job: FineTuningJob,
        required_credits: float,
        request: CreditDeductRequest
) -> CreditHistoryResponse:
    """Process credit deduction transaction."""
    # Deduct credits
    user.credits_balance -= required_credits
    job.num_tokens = request.usage_amount

    # First, see if we already deducted credits for this job
    credit_record = await billing_queries.get_credit_record(
        db,
        user.id,
        str(job.id),
        BillingTransactionType.FINE_TUNING_JOB
    )
    # If we did, return success response with existing record
    if credit_record:
        return CreditHistoryResponse.from_orm(credit_record)

    # Record deduction
    credit_record = BillingCredit(
        user_id=user.id,
        credits=-required_credits,
        transaction_id=str(job.id),
        transaction_type=BillingTransactionType.FINE_TUNING_JOB
    )
    db.add(credit_record)

    # Record usage
    usage_record = Usage(
        user_id=user.id,
        usage_amount=request.usage_amount,
        usage_unit=request.usage_unit,
        cost=required_credits,
        service_name=request.service_name,
        fine_tuning_job_id=job.id
    )
    db.add(usage_record)

    await db.commit()
    await db.refresh(credit_record)

    logger.info(f"Deducted {required_credits} credits for user: {user.id}, job: {job.id}")
    return CreditHistoryResponse.from_orm(credit_record)


async def handle_insufficient_credits(
        db: AsyncSession,
        user: User,
        job: FineTuningJob,
        required_credits: float,
        request: CreditDeductRequest
) -> CreditHistoryResponse:
    """Handle case where user has insufficient credits with retry."""
    credits_to_charge = required_credits - user.credits_balance

    # Attempt offline charge
    if not stripe_charge_offline(user, float(credits_to_charge)):
        raise PaymentNeededError(f"Failed to charge user: {user.id}", logger)

    logger.info(f"Recharged user: {user.id} with {credits_to_charge} credits")
    await asyncio.sleep(20)  # Allow time for payment processing
    await db.refresh(user)

    # Retry deduction
    return await deduct_credits(request, db, retry=False)


async def handle_stripe_webhook(request: Request, db: AsyncSession):
    """Handle Stripe webhook callbacks."""
    # Verify webhook signature
    payload = await request.body()
    sig_header = request.headers['stripe-signature']

    try:
        event = stripe.Webhook.construct_event(
            payload, sig_header, config.stripe_webhook_secret
        )
    except Exception as e:
        logger.error(f"Invalid Stripe webhook: {str(e)}")
        return {"status": "error"}

    try:
        if event["type"] == "charge.succeeded":
            await handle_successful_charge(db, event["data"]["object"])
        elif event["type"] == "customer.updated":
            await handle_customer_update(db, event["data"]["object"])
    except Exception as e:
        logger.error(f"Error processing webhook: {str(e)}")
        return {"status": "error"}

    return {"status": "success"}


async def get_credit_history(
        db: AsyncSession,
        user_id: UUID,
        start_date_str: str,
        end_date_str: str,
        page: int = 1,
        items_per_page: int = 20
) -> tuple[list[CreditHistoryResponse], Pagination]:
    """
    Get credit history for a user with date filtering and pagination.

    Args:
        db: Database session
        user_id: User ID
        start_date_str: Start date in YYYY-MM-DD format
        end_date_str: End date in YYYY-MM-DD format
        page: Page number
        items_per_page: Number of items per page

    Returns:
        Tuple of list of credit history records and pagination info

    Raises:
        BadRequestError: If dates are invalid or end date is before start date
    """
    # Parse and validate dates
    try:
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date()
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
    except ValueError:
        raise BadRequestError(
            "Invalid date format. Please use YYYY-MM-DD format"
        )

    if end_date < start_date:
        raise BadRequestError(
            f"End date ({end_date}) must be after start date ({start_date})"
        )

    # Calculate pagination
    offset = (page - 1) * items_per_page

    # Get total count for pagination
    total_count = await billing_queries.count_credit_history(
        db,
        user_id,
        start_date,
        end_date
    )

    # Calculate total pages
    total_pages = (total_count + items_per_page - 1) // items_per_page

    # Create pagination object
    pagination = Pagination(
        total_pages=total_pages,
        current_page=page,
        items_per_page=items_per_page
    )

    # Get credit history records
    credits = await billing_queries.get_credit_history(
        db,
        user_id,
        start_date,
        end_date,
        offset,
        items_per_page
    )

    # Convert to response objects
    credit_responses = [
        CreditHistoryResponse.from_orm(credit)
        for credit in credits
    ]

    logger.info(
        f"Retrieved {len(credit_responses)} credit history records for user: "
        f"{user_id} between {start_date} and {end_date}"
    )

    return credit_responses, pagination


async def handle_successful_charge(db: AsyncSession, charge_data: dict) -> None:
    """
    Handle a successful Stripe charge by adding credits to user's account.

    Args:
        db: Database session
        charge_data: Stripe charge event data

    Raises:
        ServerError: If user not found or credit addition fails
    """
    try:
        # Extract charge information
        stripe_customer_id = charge_data["customer"]
        amount_cents = charge_data["amount_captured"]
        transaction_id = charge_data["id"]

        # Convert cents to dollars and then to credits (1:1 ratio for dollars to credits)
        amount_dollars = amount_cents / 100

        # Find user by Stripe customer ID
        user = await user_queries.get_user_by_stripe_customer_id(db, stripe_customer_id)
        if not user:
            raise ServerError(
                f"User not found for Stripe customer: {stripe_customer_id}",
                logger
            )

        # Add credits to user's balance
        user.credits_balance += amount_dollars

        # Record the credit addition
        credit_record = BillingCredit(
            user_id=user.id,
            credits=amount_dollars,
            transaction_id=transaction_id,
            transaction_type=BillingTransactionType.STRIPE_CHECKOUT
        )
        db.add(credit_record)

        await db.commit()
        logger.info(
            f"Added {amount_dollars} credits to user {user.id} "
            f"from Stripe charge {transaction_id}"
        )

    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to process successful charge: {str(e)}")
        raise ServerError(f"Failed to process charge: {str(e)}", logger)


async def handle_customer_update(db: AsyncSession, customer_data: dict) -> None:
    """
    Handle Stripe customer update events, particularly payment method changes.

    Args:
        db: Database session
        customer_data: Stripe customer update event data

    Raises:
        ServerError: If user not found or update fails
    """
    try:
        # Extract customer information
        stripe_customer_id = customer_data["id"]
        default_payment_method = customer_data["invoice_settings"]["default_payment_method"]

        # Find user by Stripe customer ID
        user = await user_queries.get_user_by_stripe_customer_id(db, stripe_customer_id)
        if not user:
            raise ServerError(
                f"User not found for Stripe customer: {stripe_customer_id}",
                logger
            )

        # Update user's payment method
        user.stripe_payment_method_id = default_payment_method
        await db.commit()

        logger.info(
            f"Updated payment method for user {user.id} "
            f"to {default_payment_method}"
        )

    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to process customer update: {str(e)}")
        raise ServerError(f"Failed to process customer update: {str(e)}", logger)


async def calculate_required_credits(
        usage_amount: int,
        usage_unit: str,
        base_model_name: str
) -> float:
    """
    Calculate required credits based on usage amount and model type.

    Args:
        usage_amount: Amount of usage (e.g., number of tokens)
        usage_unit: Unit of usage (e.g., TOKEN)
        base_model_name: Name of the base model

    Returns:
        Required credits amount

    Raises:
        BadRequestError: If pricing logic not implemented for the model or unit
    """
    if usage_unit != UsageUnit.TOKEN:
        raise BadRequestError(
            f"Pricing not implemented for usage unit: {usage_unit}"
        )

    # Convert token count to millions for pricing
    tokens_in_millions = usage_amount / 1_000_000

    # Define pricing tiers based on model
    model_pricing = {
        'llm_llama3_1_8b': 2.0,  # $ per million tokens
        'llm_llama3_1_70b': 10.0,
        'llm_llama3_2_1b': 1.0,
        'llm_llama3_2_3b': 1.0,
        'llm_llama3_3_70b': 10.0,
        'llm_dummy': 0.1,
    }

    # Get price per million tokens for the model
    price_per_million = model_pricing.get(base_model_name)
    if price_per_million is None:
        raise BadRequestError(
            f"Pricing not implemented for base model: {base_model_name}"
        )

    # Calculate total credits needed (1 credit = $1)
    required_credits = tokens_in_millions * price_per_million

    logger.info(
        f"Calculated credits for {usage_amount} tokens "
        f"using {base_model_name}: {required_credits}"
    )

    return required_credits


async def add_credits_to_user(
        db: AsyncSession,
        user_id: UUID,
        amount: float,
        transaction_id: str,
        transaction_type: BillingTransactionType
) -> CreditHistoryResponse:
    """
    Add credits to a user's account and record the transaction.

    Args:
        db: Database session
        user_id: User ID
        amount: Amount of credits to add
        transaction_id: Unique transaction identifier
        transaction_type: Type of credit transaction

    Returns:
        Credit history record

    Raises:
        UserNotFoundError: If user not found
        BadRequestError: If transaction already exists
        ServerError: If credit addition fails
    """
    # Verify user exists
    user = await user_queries.get_user_by_id(db, user_id)
    if not user:
        raise UserNotFoundError(f"User not found: {user_id}", logger)

    # Check for existing transaction
    existing_credit = await billing_queries.get_credit_record(
        db,
        user_id,
        transaction_id,
        transaction_type
    )
    if existing_credit:
        raise BadRequestError(
            f"Transaction already exists: {transaction_id}",
            logger
        )

    try:
        # Add credits to user's balance
        user.credits_balance += amount

        # Record the credit addition
        credit_record = BillingCredit(
            user_id=user_id,
            credits=amount,
            transaction_id=transaction_id,
            transaction_type=transaction_type
        )
        db.add(credit_record)

        # Commit changes
        await db.commit()
        await db.refresh(credit_record)

        logger.info(
            f"Added {amount} credits to user {user_id} "
            f"(transaction: {transaction_id}, type: {transaction_type})"
        )

        return CreditHistoryResponse.from_orm(credit_record)

    except Exception as e:
        await db.rollback()
        raise ServerError(f"Failed to add credits: {str(e)}", logger)

================
File: services/dataset.py
================
from uuid import UUID

from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.common import sanitize_filename
from app.core.config_manager import config
from app.core.constants import DatasetStatus
from app.core.exceptions import (
    DatasetAlreadyExistsError,
    DatasetNotFoundError,
)
from app.core.storage import upload_file, delete_file
from app.core.utils import setup_logger
from app.models.dataset import Dataset
from app.queries import datasets as dataset_queries
from app.schemas.common import Pagination
from app.schemas.dataset import DatasetCreate, DatasetResponse, DatasetUpdate

# Set up logger
logger = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


def get_dataset_bucket() -> str:
    """Get the dataset bucket."""
    return f'lum-{config.env_name}-{config.gcs_datasets_bucket}'


async def create_dataset(db: AsyncSession, user_id: UUID, dataset: DatasetCreate) -> DatasetResponse:
    """Create a new dataset."""
    # Check if a dataset with the same name already exists
    existing_dataset = await dataset_queries.get_dataset_by_name(db, user_id, dataset.name)
    if existing_dataset:
        raise DatasetAlreadyExistsError(f"A dataset with the name '{dataset.name}' already "
                                        f"exists for user {user_id}", logger)

    # Sanitize the filename
    original_filename = dataset.file.filename
    sanitized_filename = sanitize_filename(original_filename)
    dataset.file.filename = sanitized_filename

    # Upload the dataset file to storage
    file_name = await upload_file(get_dataset_bucket(), '', dataset.file, user_id)

    try:
        # Create the dataset record
        db_dataset = Dataset(
            user_id=user_id,
            name=dataset.name,
            description=dataset.description,
            file_name=file_name,
            file_size=dataset.file.size,
            status=DatasetStatus.UPLOADED
        )
        db.add(db_dataset)
        await db.commit()
        await db.refresh(db_dataset)

        logger.info(f"Created dataset: {db_dataset.id} for user: {user_id}")
        return DatasetResponse.from_orm(db_dataset)

    except SQLAlchemyError as e:
        # If there's an SQL error, delete the uploaded file
        await delete_file(get_dataset_bucket(), '', file_name, user_id)
        await db.rollback()
        raise e


async def get_datasets(
        db: AsyncSession,
        user_id: UUID,
        page: int = 1,
        items_per_page: int = 20
) -> tuple[list[DatasetResponse], Pagination]:
    """Get all datasets for a user with pagination."""
    offset = (page - 1) * items_per_page

    # Get total count and paginated results
    total_count = await dataset_queries.count_datasets(db, user_id)
    datasets = await dataset_queries.list_datasets(db, user_id, offset, items_per_page)

    # Calculate pagination
    total_pages = (total_count + items_per_page - 1) // items_per_page
    pagination = Pagination(
        total_pages=total_pages,
        current_page=page,
        items_per_page=items_per_page,
    )

    # Create response objects
    dataset_responses = [DatasetResponse.from_orm(dataset) for dataset in datasets]

    logger.info(f"Retrieved datasets for user: {user_id}, page: {page}")
    return dataset_responses, pagination


async def get_dataset(db: AsyncSession, user_id: UUID, dataset_name: str) -> DatasetResponse:
    """Get a specific dataset."""
    dataset = await dataset_queries.get_dataset_by_name(db, user_id, dataset_name)
    if not dataset:
        raise DatasetNotFoundError(f"Dataset not found: {dataset_name} for user: {user_id}", logger)

    logger.info(f"Retrieved dataset: {dataset_name} for user: {user_id}")
    return DatasetResponse.from_orm(dataset)


async def update_dataset(db: AsyncSession, user_id: UUID, dataset_name: str,
                         dataset_update: DatasetUpdate) -> DatasetResponse:
    """Update a dataset."""
    db_dataset = await dataset_queries.get_dataset_by_name(db, user_id, dataset_name)
    if not db_dataset:
        raise DatasetNotFoundError(f"Dataset not found: {dataset_name} for user: {user_id}", logger)

    new_db_dataset = await dataset_queries.get_dataset_by_name(db, user_id, dataset_update.name)
    if new_db_dataset:
        raise DatasetAlreadyExistsError(f"A dataset with the name '{dataset_update.name}' already "
                                        f"exists for user {user_id}", logger)

    # Update the dataset fields
    update_data = dataset_update.dict(exclude_unset=True)
    for field, value in update_data.items():
        setattr(db_dataset, field, value)

    await db.commit()
    await db.refresh(db_dataset)

    logger.info(f"Updated dataset: {dataset_name} for user: {user_id}")
    return DatasetResponse.from_orm(db_dataset)


async def delete_dataset(db: AsyncSession, user_id: UUID, dataset_name: str) -> None:
    """Delete a dataset."""
    db_dataset = await dataset_queries.get_dataset_by_name(db, user_id, dataset_name)
    if not db_dataset:
        raise DatasetNotFoundError(f"Dataset not found: {dataset_name} for user: {user_id}", logger)

    try:
        # Delete the file from storage first
        await delete_file(get_dataset_bucket(), '', db_dataset.file_name, user_id)

        # Mark the dataset as deleted in the database
        db_dataset.status = DatasetStatus.DELETED
        await db.commit()

        logger.info(f"Deleted dataset: {dataset_name} for user: {user_id}")
    except Exception as e:
        await db.rollback()
        raise e

================
File: services/fine_tuned_model.py
================
from typing import Dict, Any
from uuid import UUID

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config_manager import config
from app.core.exceptions import FineTunedModelNotFoundError
from app.core.utils import setup_logger
from app.queries import fine_tuned_models as ft_models_queries
from app.queries import fine_tuning as ft_jobs_queries
from app.schemas.common import Pagination
from app.schemas.model import FineTunedModelResponse

logger = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


async def get_fine_tuned_models(
        db: AsyncSession,
        user_id: UUID,
        page: int = 1,
        items_per_page: int = 20
) -> tuple[list[FineTunedModelResponse], Pagination]:
    """Get all fine-tuned models for a user with pagination."""
    offset = (page - 1) * items_per_page

    # Get total count and paginated results
    total_count = await ft_models_queries.count_models(db, user_id)
    results = await ft_models_queries.list_models(db, user_id, offset, items_per_page)

    # Calculate pagination
    total_pages = (total_count + items_per_page - 1) // items_per_page
    pagination = Pagination(
        total_pages=total_pages,
        current_page=page,
        items_per_page=items_per_page,
    )

    # Create response objects
    models = []
    for model, job_name in results:
        model_dict = model.__dict__
        model_dict['fine_tuning_job_name'] = job_name
        models.append(FineTunedModelResponse(**model_dict))

    logger.info(f"Retrieved {len(models)} fine-tuned models for user: {user_id}, page: {page}")
    return models, pagination


async def get_fine_tuned_model(
        db: AsyncSession,
        user_id: UUID,
        model_name: str
) -> FineTunedModelResponse:
    """Get detailed information about a specific fine-tuned model."""
    result = await ft_models_queries.get_model_by_name(db, user_id, model_name)
    if not result:
        raise FineTunedModelNotFoundError(f"Fine-tuned model not found: {model_name} for user: {user_id}", logger)

    model, job_name = result
    model_dict = model.__dict__
    model_dict['fine_tuning_job_name'] = job_name

    logger.info(f"Retrieved fine-tuned model: {model_name} for user: {user_id}")
    return FineTunedModelResponse(**model_dict)


async def create_fine_tuned_model(
        db: AsyncSession,
        job_id: UUID,
        user_id: UUID,
        artifacts: Dict[str, Any]
) -> bool:
    """
    Create a fine-tuned model record from job artifacts.

    Args:
        db: Database session
        job_id: Fine-tuning job ID
        user_id: User ID
        artifacts: Model artifacts (weights, configs, etc.)

    Returns:
        True if model was created, False if skipped

    Note:
        This is idempotent - it won't create duplicate models
        for the same job
    """
    # Check if the job exists and belongs to the user
    job = await ft_jobs_queries.get_job_by_id(db, job_id, user_id)
    if not job:
        logger.warning(f"Cannot create model: Job {job_id} not found for user {user_id}")
        return False

    # Check if a model already exists for this job
    existing_model = await ft_models_queries.get_existing_model(db, job_id, user_id)
    if existing_model:
        logger.warning(f"Model already exists for job {job_id}: {existing_model.id}")
        return True

    try:
        # Create new model
        model = await ft_models_queries.create_model(
            db,
            job_id,
            user_id,
            f"{job.name}_model",
            artifacts
        )
        await db.commit()

        logger.info(f"Created fine-tuned model for job {job_id}: {model.id}")
        return True

    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to create model for job {job_id}: {str(e)}")
        return False

================
File: services/fine_tuning.py
================
from typing import List
from uuid import UUID

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config_manager import config
from app.core.constants import (
    FineTuningJobStatus,
    FineTuningJobType, FineTunedModelStatus, ComputeProvider, DatasetStatus
)
from app.core.exceptions import (
    BaseModelNotFoundError,
    DatasetNotFoundError,
    FineTuningJobAlreadyExistsError,
    ForbiddenError, FineTuningJobNotFoundError, BadRequestError, FineTuningJobCreationError
)
from app.core.scheduler_client import start_fine_tuning_job, stop_fine_tuning_job
from app.core.utils import setup_logger
from app.models.fine_tuning_job import FineTuningJob
from app.models.fine_tuning_job_detail import FineTuningJobDetail
from app.models.user import User
from app.queries import datasets as dataset_queries
from app.queries import fine_tuned_models as ft_models_queries
from app.queries import fine_tuning as ft_queries
from app.queries import models as model_queries
from app.schemas.common import Pagination
from app.schemas.fine_tuning import (
    FineTuningJobCreate,
    FineTuningJobDetailResponse, FineTuningJobResponse
)

logger = setup_logger(__name__)


async def create_fine_tuning_job(
        db: AsyncSession,
        user: User,
        job: FineTuningJobCreate
) -> FineTuningJobDetailResponse:
    """Create a new fine-tuning job."""

    # Validate user status
    if not user.email_verified:
        raise ForbiddenError("Email verification required", logger)

    if user.credits_balance < config.fine_tuning_job_min_credits:
        raise ForbiddenError(f"Insufficient credits. Required: {config.fine_tuning_job_min_credits}", logger)

    # Validate base model
    base_model = await model_queries.get_base_model_by_name(db, job.base_model_name)
    if not base_model:
        raise BaseModelNotFoundError(f"Base model not found: {job.base_model_name}", logger)
    if base_model.status != FineTunedModelStatus.ACTIVE:
        raise BaseModelNotFoundError(f"Base model is not active: {job.base_model_name}", logger)

    # Validate dataset
    dataset = await dataset_queries.get_dataset_by_name(db, user.id, job.dataset_name)
    if not dataset or dataset.status not in (DatasetStatus.UPLOADED, DatasetStatus.VALIDATED):
        raise DatasetNotFoundError(f"Dataset not found or deleted: {job.dataset_name}", logger)

    # Check for duplicate job name
    existing_job = await ft_queries.get_job_with_details(db, user.id, job.name)
    if existing_job:
        raise FineTuningJobAlreadyExistsError(f"Job name already exists: {job.name}", logger)

    try:
        # Prepare job parameters
        params = job.parameters.copy()
        params['use_lora'] = job.type in (FineTuningJobType.LORA, FineTuningJobType.QLORA)
        params['use_qlora'] = job.type == FineTuningJobType.QLORA

        # Create job record
        db_job = FineTuningJob(
            user_id=user.id,
            name=job.name,
            type=job.type,
            provider=job.provider,
            base_model_id=base_model.id,
            dataset_id=dataset.id,
            status=FineTuningJobStatus.NEW
        )
        db.add(db_job)
        await db.flush()

        # Create job details
        db_job_detail = FineTuningJobDetail(
            fine_tuning_job_id=db_job.id,
            parameters=params,
            metrics={},
            timestamps={
                "new": None, "queued": None, "running": None,
                "stopping": None, "stopped": None,
                "completed": None, "failed": None
            }
        )
        db.add(db_job_detail)
        await db.commit()
        await db.refresh(db_job)

        # Start the job via scheduler
        await start_fine_tuning_job(db, db_job.id, user.id)

        # Prepare response
        response_data = {
            **db_job.__dict__,
            'base_model_name': base_model.name,
            'dataset_name': dataset.name,
            'parameters': db_job_detail.parameters,
            'metrics': db_job_detail.metrics,
            'timestamps': db_job_detail.timestamps
        }

        logger.info(f"Created fine-tuning job: {db_job.id} for user: {user.id}")
        return FineTuningJobDetailResponse(**response_data)

    except Exception as e:
        await db.rollback()
        raise e


async def get_fine_tuning_jobs(
        db: AsyncSession,
        user_id: UUID,
        page: int = 1,
        items_per_page: int = 20
) -> tuple[list[FineTuningJobResponse], Pagination]:
    """Get all fine-tuning jobs for a user with pagination."""
    offset = (page - 1) * items_per_page

    # Get total count and paginated results
    total_count = await ft_queries.count_jobs(db, user_id)
    results = await ft_queries.list_jobs(db, user_id, offset, items_per_page)

    # Calculate pagination
    total_pages = (total_count + items_per_page - 1) // items_per_page
    pagination = Pagination(
        total_pages=total_pages,
        current_page=page,
        items_per_page=items_per_page,
    )

    # Create response objects
    jobs = []
    for job, base_model_name, dataset_name in results:
        job_dict = job.__dict__
        job_dict['base_model_name'] = base_model_name
        job_dict['dataset_name'] = dataset_name
        jobs.append(FineTuningJobResponse(**job_dict))

    logger.info(f"Retrieved {len(jobs)} fine-tuning jobs for user: {user_id}, page: {page}")
    return jobs, pagination


async def get_fine_tuning_job(
        db: AsyncSession,
        user_id: UUID,
        job_name: str
) -> FineTuningJobDetailResponse:
    """Get detailed information about a specific fine-tuning job."""
    result = await ft_queries.get_job_with_details(db, user_id, job_name)
    if not result:
        raise FineTuningJobNotFoundError(f"Job not found: {job_name}", logger)

    job, detail, base_model_name, dataset_name = result
    response_data = {
        **job.__dict__,
        'base_model_name': base_model_name,
        'dataset_name': dataset_name,
        'parameters': detail.parameters,
        'metrics': detail.metrics,
        'timestamps': detail.timestamps
    }

    logger.info(f"Retrieved fine-tuning job: {job_name} for user: {user_id}")
    return FineTuningJobDetailResponse(**response_data)


async def cancel_fine_tuning_job(
        db: AsyncSession,
        user_id: UUID,
        job_name: str
) -> FineTuningJobDetailResponse:
    """Cancel a specific fine-tuning job."""
    # Get job details
    result = await ft_queries.get_job_with_details(db, user_id, job_name)
    if not result:
        raise FineTuningJobNotFoundError(f"Job not found: {job_name}", logger)

    job, detail, base_model_name, dataset_name = result

    # Validate job can be cancelled
    if job.provider == ComputeProvider.LUM:
        raise BadRequestError(f"Cannot cancel job running on {ComputeProvider.LUM} protocol", logger)

    if job.status != FineTuningJobStatus.RUNNING:
        raise BadRequestError(f"Job cannot be cancelled in state: {job.status.value}", logger)

    try:
        # Request job cancellation from scheduler
        await stop_fine_tuning_job(job.id, user_id)

        # Update job status
        job.status = FineTuningJobStatus.STOPPING
        await db.commit()
        await db.refresh(job)

        # Prepare response
        response_data = {
            **job.__dict__,
            'base_model_name': base_model_name,
            'dataset_name': dataset_name,
            'parameters': detail.parameters,
            'metrics': detail.metrics,
            'timestamps': detail.timestamps
        }

        logger.info(f"Cancelled fine-tuning job: {job_name} for user: {user_id}")
        return FineTuningJobDetailResponse(**response_data)

    except Exception as e:
        await db.rollback()
        raise e


async def delete_fine_tuning_job(
        db: AsyncSession,
        user_id: UUID,
        job_name: str
) -> None:
    """Mark a fine-tuning job and its associated model as deleted."""
    # Get job with fine-tuned model
    result = await ft_queries.get_job_with_details(db, user_id, job_name)
    if not result:
        raise FineTuningJobNotFoundError(f"Job not found: {job_name}", logger)

    job, detail, _, _ = result

    try:
        # Mark job as deleted
        job.status = FineTuningJobStatus.DELETED

        # If there's an associated fine-tuned model, mark it as deleted too
        fine_tuned_model = await ft_models_queries.get_existing_model(db, job.id, user_id)
        if fine_tuned_model:
            fine_tuned_model.status = FineTunedModelStatus.DELETED

        await db.commit()
        logger.info(f"Marked fine-tuning job as deleted: {job_name} for user: {user_id}")

    except Exception as e:
        await db.rollback()
        raise e


async def update_job_progress(
        db: AsyncSession,
        job: FineTuningJob,
        progress: dict
) -> bool:
    """Update job progress information."""

    # Ignore outdated progress updates
    if progress['current_step'] <= (job.current_step or -1):
        return True

    try:
        # Update job progress
        job.current_step = progress['current_step']
        job.total_steps = progress['total_steps']
        job.current_epoch = progress['current_epoch']
        job.total_epochs = progress['total_epochs']

        await db.commit()
        logger.info(f"Updated progress for job: {job.id}, step: {progress['current_step']}")
        return True

    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to update job progress: {str(e)}")
        return False


async def get_jobs_for_status_update(
        db: AsyncSession,
        include_recent_completed: bool = True
) -> List[FineTuningJob]:
    """Get jobs that need status updates."""
    non_terminal_statuses = [
        FineTuningJobStatus.NEW,
        FineTuningJobStatus.QUEUED,
        FineTuningJobStatus.RUNNING,
        FineTuningJobStatus.STOPPING
    ]

    completed_within_minutes = 10 if include_recent_completed else None

    jobs = await ft_queries.get_non_terminal_jobs(
        db,
        statuses=non_terminal_statuses,
        completed_within_minutes=completed_within_minutes
    )

    return jobs

================
File: services/model.py
================
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config_manager import config
from app.core.exceptions import BaseModelNotFoundError
from app.core.utils import setup_logger
from app.queries import models as model_queries
from app.schemas.common import Pagination
from app.schemas.model import BaseModelResponse

logger = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


async def get_base_models(
        db: AsyncSession,
        page: int = 1,
        items_per_page: int = 20
) -> tuple[list[BaseModelResponse], Pagination]:
    """Get all available base LLM models with pagination."""
    offset = (page - 1) * items_per_page

    # Get total count and paginated results
    total_count = await model_queries.count_base_models(db)
    models = await model_queries.list_base_models(db, offset, items_per_page)

    # Calculate pagination
    total_pages = (total_count + items_per_page - 1) // items_per_page
    pagination = Pagination(
        total_pages=total_pages,
        current_page=page,
        items_per_page=items_per_page,
    )

    # Create response objects
    model_responses = [BaseModelResponse.from_orm(model) for model in models]

    logger.info(f"Retrieved {len(model_responses)} base models, page: {page}")
    return model_responses, pagination


async def get_base_model(db: AsyncSession, model_name: str) -> BaseModelResponse:
    """Get detailed information about a specific base model."""
    model = await model_queries.get_base_model_by_name(db, model_name)
    if not model:
        raise BaseModelNotFoundError(f"Base model not found: {model_name}", logger)

    logger.info(f"Retrieved base model: {model_name}")
    return BaseModelResponse.from_orm(model)

================
File: services/usage.py
================
from datetime import datetime
from uuid import UUID

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.exceptions import BadRequestError
from app.core.utils import setup_logger
from app.queries import usage as usage_queries
from app.schemas.common import Pagination
from app.schemas.usage import UsageRecordResponse, TotalCostResponse

logger = setup_logger(__name__)


async def get_usage_records(
        db: AsyncSession,
        user_id: UUID,
        start_date_str: str,
        end_date_str: str,
        page: int = 1,
        items_per_page: int = 20
) -> tuple[list[UsageRecordResponse], Pagination]:
    """
    Get usage records for a user with pagination.

    Args:
        db: Database session
        user_id: User ID
        start_date_str: Start date in YYYY-MM-DD format
        end_date_str: End date in YYYY-MM-DD format
        page: Page number
        items_per_page: Items per page

    Returns:
        Tuple of usage records and pagination info

    Raises:
        BadRequestError: If dates are invalid
    """
    # Parse and validate dates
    try:
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date()
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
    except ValueError:
        raise BadRequestError(
            "Invalid date format. Please use YYYY-MM-DD format"
        )

    if end_date < start_date:
        raise BadRequestError(
            f"End date ({end_date}) must be after start date ({start_date})"
        )

    # Calculate pagination
    offset = (page - 1) * items_per_page

    # Get total count and records
    total_count = await usage_queries.count_usage_records(
        db, user_id, start_date, end_date
    )

    usage_records = await usage_queries.get_usage_records(
        db, user_id, start_date, end_date, offset, items_per_page
    )

    # Prepare pagination
    total_pages = (total_count + items_per_page - 1) // items_per_page
    pagination = Pagination(
        total_pages=total_pages,
        current_page=page,
        items_per_page=items_per_page
    )

    # Create response objects
    usage_responses = []
    for usage, job_name in usage_records:
        usage_dict = usage.__dict__
        usage_dict['fine_tuning_job_name'] = job_name
        usage_responses.append(UsageRecordResponse(**usage_dict))

    logger.info(
        f"Retrieved {len(usage_responses)} usage records for user: {user_id} "
        f"between {start_date} and {end_date}"
    )

    return usage_responses, pagination


async def get_total_cost(
        db: AsyncSession,
        user_id: UUID,
        start_date_str: str,
        end_date_str: str
) -> TotalCostResponse:
    """
    Get total cost for a period.

    Args:
        db: Database session
        user_id: User ID
        start_date_str: Start date in YYYY-MM-DD format
        end_date_str: End date in YYYY-MM-DD format

    Returns:
        Total cost information

    Raises:
        BadRequestError: If dates are invalid
    """
    # Parse and validate dates
    try:
        start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date()
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date()
    except ValueError:
        raise BadRequestError(
            "Invalid date format. Please use YYYY-MM-DD format"
        )

    if end_date < start_date:
        raise BadRequestError(
            f"End date ({end_date}) must be after start date ({start_date})"
        )

    # Get total cost
    total_cost = await usage_queries.get_total_cost(
        db, user_id, start_date, end_date
    )

    logger.info(
        f"Calculated total cost for user {user_id}: {total_cost} "
        f"between {start_date} and {end_date}"
    )

    return TotalCostResponse(
        start_date=start_date,
        end_date=end_date,
        total_cost=total_cost
    )

================
File: services/user.py
================
from uuid import UUID

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config_manager import config
from app.core.constants import UserStatus, BillingTransactionType
from app.core.exceptions import UserNotFoundError
from app.core.stripe_client import create_stripe_customer
from app.core.utils import setup_logger
from app.models.user import User
from app.queries import users as user_queries
from app.schemas.user import UserUpdate, UserResponse
from app.services.billing import add_credits_to_user

# Set up logger
logger = setup_logger(__name__, add_stdout=config.log_stdout, log_level=config.log_level)


async def update_user(db: AsyncSession, user_id: UUID, user_update: UserUpdate) -> UserResponse:
    """Update a user's information."""
    logger.info(f"Attempting to update user: {user_id}")

    # Get user
    user = await user_queries.get_user_by_id(db, user_id)
    if not user:
        raise UserNotFoundError(f"User with ID {user_id} not found", logger)

    # Update the user's information
    update_data = user_update.dict(exclude_unset=True)
    for field, value in update_data.items():
        setattr(user, field, value)

    await db.commit()
    await db.refresh(user)

    logger.info(f"Successfully updated user: {user_id}")
    return UserResponse.from_orm(user)


async def deactivate_user(db: AsyncSession, user_id: UUID) -> None:
    """Set a user's status to inactive."""
    logger.info(f"Attempting to deactivate user: {user_id}")

    # Get user
    user = await user_queries.get_user_by_id(db, user_id)
    if not user:
        raise UserNotFoundError(f"User with ID {user_id} not found", logger)

    # Set status to inactive
    user.status = UserStatus.INACTIVE
    await db.commit()

    logger.info(f"Successfully deactivated user: {user_id}")


async def create_user(db: AsyncSession, name: str, email: str,
                      auth0_user_id: str, email_verified: bool) -> User:
    """Create a new user."""
    try:
        # Create the new user
        db_user = User(
            email=email,
            name=name,
            auth0_user_id=auth0_user_id,
            email_verified=email_verified
        )
        db.add(db_user)
        await db.commit()
        await db.refresh(db_user)

        # Add new user credits
        free_credits = float(config.new_user_credits)
        if config.new_user_credits:
            await add_credits_to_user(
                db, db_user.id, free_credits,
                "NEW_USER_CREDIT", BillingTransactionType.NEW_USER_CREDIT
            )

        # Create a Stripe customer
        await create_stripe_customer(db, db_user)

        logger.info(f"Successfully created new user with ID: {db_user.id}")
        return db_user

    except Exception as e:
        await db.rollback()
        raise e

================
File: tasks/api_key_cleanup.py
================
from typing import Optional

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import AsyncSessionLocal
from app.core.utils import setup_logger
from app.queries import api_keys as api_key_queries

logger = setup_logger(__name__)


async def cleanup_expired_api_keys(db: Optional[AsyncSession] = None) -> None:
    """Wrapper to _cleanup_expired_api_keys that handles database session."""
    if db is None:
        async with AsyncSessionLocal() as db:
            await _cleanup_expired_api_keys(db)
    else:
        await _cleanup_expired_api_keys(db)


async def _cleanup_expired_api_keys(db: AsyncSession) -> None:
    """Mark expired API keys as EXPIRED in a single transaction."""
    try:
        # Update expired keys to EXPIRED status
        updated_count = await api_key_queries.mark_expired_keys(db)
        await db.commit()

        logger.info(f"Marked {updated_count} API keys as expired")

    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to cleanup expired API keys: {str(e)}")

================
File: tasks/job_status_updater.py
================
from datetime import timedelta
from typing import List, Dict, Any, Optional
from uuid import UUID

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.constants import FineTuningJobStatus
from app.core.database import AsyncSessionLocal
from app.core.scheduler_client import fetch_job_details
from app.core.utils import setup_logger
from app.models.fine_tuning_job import FineTuningJob
from app.queries import fine_tuning as ft_queries
from app.queries.common import now_utc
from app.services.fine_tuned_model import create_fine_tuned_model
from app.services.fine_tuning import update_job_progress

logger = setup_logger(__name__)

# Map scheduler statuses to our internal statuses
STATUS_MAPPING = {
    "WAIT_FOR_VM": FineTuningJobStatus.QUEUED,
    "FOUND_VM": FineTuningJobStatus.QUEUED,
    "DETACHED_VM": FineTuningJobStatus.QUEUED,
}


async def update_job_statuses(db: Optional[AsyncSession] = None) -> None:
    """Wrapper to _update_job_statuses that handles database session."""
    if db is None:
        async with AsyncSessionLocal() as db:
            await _update_job_statuses(db)
    else:
        await _update_job_statuses(db)


async def _update_job_statuses(db: AsyncSession) -> None:
    """Update the status of all non-terminal jobs."""
    try:
        # Get jobs that need updates
        jobs = await _get_jobs_for_update(db)
        if not jobs:
            logger.info("No jobs found for status update")
            return

        # Group jobs by user for scheduler API
        jobs_by_user = _group_jobs_by_user(jobs)

        # Update each group of jobs
        for user_id, job_ids in jobs_by_user.items():
            await _update_job_group(db, user_id, job_ids)

    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to update job statuses: {str(e)}")


async def _get_jobs_for_update(db: AsyncSession) -> List[FineTuningJob]:
    """Get jobs that need status updates."""
    non_terminal_statuses = [
        FineTuningJobStatus.NEW,
        FineTuningJobStatus.QUEUED,
        FineTuningJobStatus.RUNNING,
        FineTuningJobStatus.STOPPING
    ]

    recent_completed_cutoff = now_utc() - timedelta(minutes=10)

    return await ft_queries.get_jobs_for_status_update(
        db,
        non_terminal_statuses,
        recent_completed_cutoff
    )


def _group_jobs_by_user(jobs: List[FineTuningJob]) -> Dict[UUID, List[UUID]]:
    """Group jobs by user ID for efficient scheduler API calls."""
    jobs_by_user = {}
    for job in jobs:
        if job.user_id not in jobs_by_user:
            jobs_by_user[job.user_id] = []
        jobs_by_user[job.user_id].append(job.id)
    return jobs_by_user


async def _update_job_group(
        db: AsyncSession,
        user_id: UUID,
        job_ids: List[UUID]
) -> None:
    """Update a group of jobs for a single user."""
    try:
        # Get updates from scheduler
        job_updates = await fetch_job_details(user_id, job_ids)

        # Process each job update
        for update in job_updates:
            await _process_job_update(db, update, user_id)

        await db.commit()
        logger.info(f"Updated {len(job_updates)} jobs for user {user_id}")

    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to update jobs for user {user_id}: {str(e)}")


async def _process_job_update(
        db: AsyncSession,
        update: Dict[str, Any],
        user_id: UUID
) -> None:
    """Process a single job update from the scheduler."""
    job_id = UUID(update['job_id'])
    job = await ft_queries.get_job_by_id(db, job_id, user_id)
    if not job:
        logger.warning(f"Job not found for update: {job_id}, user: {user_id}")
        return

    # Update job status
    new_status = STATUS_MAPPING.get(update['status']) or update['status']
    if job.status != new_status:
        job.status = new_status
        logger.info(f"Updated status for job {job_id} to {new_status}")

    # Update timestamps
    await _update_job_timestamps(job, update['timestamps'])

    # Update progress
    await _update_job_steps(db, job, update['artifacts'])

    # Create fine-tuned model if needed
    await _check_create_model(db, job_id, user_id, update['artifacts'])


async def _update_job_timestamps(
        job: Any,
        timestamps: Dict[str, str]
) -> None:
    """Update job timestamps from scheduler data."""
    job_timestamps = job.details.timestamps.copy()

    for event, timestamp in timestamps.items():
        # Map scheduler status to API status if needed
        if event.upper() in STATUS_MAPPING:
            if timestamp:  # Only update if we have a timestamp
                mapped_status = STATUS_MAPPING[event.upper()].lower()
                job_timestamps[mapped_status] = timestamp
        else:
            # Direct update for unmapped statuses
            job_timestamps[event.lower()] = timestamp

    job.details.timestamps = job_timestamps


async def _update_job_steps(
        db: AsyncSession,
        job: FineTuningJob,
        artifacts: Dict[str, Any]
) -> None:
    """Update job progress from artifacts."""
    if not artifacts:
        return

    job_logs = artifacts.get('job_logger', [])
    max_step = 0
    max_epoch = 0
    num_steps = 0
    num_epochs = 0

    for log in job_logs:
        if log.get('operation') == 'step':
            max_step = max(max_step, log['data']['step_num'])
            max_epoch = max(max_epoch, log['data']['epoch_num'])
            num_steps = log['data']['step_len']
            num_epochs = log['data']['epoch_len']

    if job.current_step is None or max_step > job.current_step:
        progress = {
            "current_step": max_step,
            "total_steps": num_steps,
            "current_epoch": max_epoch,
            "total_epochs": num_epochs,
        }
        await update_job_progress(db, job, progress)


async def _check_create_model(
        db: AsyncSession,
        job_id: UUID,
        user_id: UUID,
        artifacts: Dict[str, Any]
) -> None:
    """Check and create fine-tuned model if weights are available."""
    if not artifacts:
        return

    for log in artifacts.get('job_logger', []):
        if log.get('operation') == 'weights':
            await create_fine_tuned_model(db, job_id, user_id, log['data'])

================
File: tasks/model_cleanup.py
================
from datetime import timedelta
from typing import Optional, Dict, Any

from gcloud.aio.storage import Storage
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import AsyncSessionLocal
from app.core.utils import setup_logger
from app.models.fine_tuned_model import FineTunedModel
from app.queries import models as model_queries
from app.queries.common import now_utc

logger = setup_logger(__name__)


async def cleanup_deleted_model_weights(db: Optional[AsyncSession] = None) -> None:
    """Wrapper to _cleanup_deleted_model_weights that handles database session."""
    if db is None:
        async with AsyncSessionLocal() as db:
            await _cleanup_deleted_model_weights(db)
    else:
        await _cleanup_deleted_model_weights(db)


async def _cleanup_deleted_model_weights(db: AsyncSession) -> None:
    """Clean up weights files from GCS for deleted models."""
    try:
        logger.info("Starting model weights cleanup")

        # Find recently deleted models (within last 3 days)
        cutoff_date = now_utc() - timedelta(days=3)
        deleted_models = await model_queries.get_deleted_models(
            db,
            cutoff_date
        )
        logger.info(f"Found {len(deleted_models)} deleted models for cleanup")

        if not deleted_models:
            return

        storage = Storage()
        for model in deleted_models:
            await _cleanup_model_weights(model, storage)

        await db.commit()
        logger.info("Model weights cleanup complete")

    except Exception as e:
        await db.rollback()
        logger.error(f"Failed to cleanup model weights: {str(e)}")


async def _cleanup_model_weights(model: FineTunedModel, storage: Storage) -> None:
    """Clean up weights for a single model."""
    if not model.artifacts:
        logger.info(f"Model {model.id} has no artifacts, skipping")
        return

    logger.info(f"Cleaning up weights for model {model.id}")

    # Extract bucket and path information
    base_url = model.artifacts['base_url']
    # Parse gs://bucket-name/path/to/file format
    bucket_name = base_url.split('/')[3]
    user_id, job_id = base_url.split('/')[4:6]

    # Delete each weight file
    for weight_file in model.artifacts.get('weight_files', []):
        weight_path = f"{user_id}/{job_id}/{weight_file}"
        try:
            await storage.delete(bucket=bucket_name, object_name=weight_path)
            gs_path = f"gs://{bucket_name}/{weight_path}"
            logger.info(f"Deleted weight file: {gs_path}")
        except Exception as e:
            # Log error and continue with next file, we don't want to stop the cleanup process
            if "404" in str(e):
                logger.warning(f"Weight file not found: {weight_path}, model {model.id}")
            else:
                logger.error(f"Error deleting weight file {weight_path}, model {model.id}: {str(e)}")

    # Update artifacts to remove weight files
    artifacts = _update_model_artifacts(model.artifacts)
    model.artifacts = artifacts

    logger.info(f"Deleted weights for model {model.id}")


def _update_model_artifacts(artifacts: Dict[str, Any]) -> Dict[str, Any]:
    """Update model artifacts to remove weight files."""
    updated_artifacts = artifacts.copy()
    updated_artifacts['weight_files'] = []
    return updated_artifacts

================
File: main.py
================
from contextlib import asynccontextmanager

import stripe
import uvicorn
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from fastapi import FastAPI
from fastapi.exceptions import RequestValidationError
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from sqlalchemy.exc import SQLAlchemyError
from starlette.middleware.sessions import SessionMiddleware
from starlette.requests import Request

from app.core.config_manager import config
from app.core.database import engine, Base
from app.core.exceptions import (
    AppException,
    app_exception_handler,
    validation_exception_handler,
    sqlalchemy_exception_handler,
    generic_exception_handler,
)
from app.routes import users, api_keys, datasets, fine_tuning, models, usage, auth0, billing
from app.tasks.api_key_cleanup import cleanup_expired_api_keys
from app.tasks.job_status_updater import update_job_statuses
from app.tasks.model_cleanup import cleanup_deleted_model_weights

# Create the background task scheduler instance
background_task_scheduler = AsyncIOScheduler()


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    # -------
    # Initialize Stripe
    stripe.api_key = config.stripe_secret_key
    # Run database migrations
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    # Add the job weights cleanup task to the background scheduler
    background_task_scheduler.add_job(cleanup_deleted_model_weights, 'interval', minutes=1)
    # Add the API key cleanup task to the background scheduler
    background_task_scheduler.add_job(cleanup_expired_api_keys, 'interval', minutes=1)
    # Add the job status updater task to the background scheduler
    if config.run_with_scheduler:
        background_task_scheduler.add_job(update_job_statuses, 'interval', seconds=15)
    # Start the background scheduler
    background_task_scheduler.start()

    yield

    # Shutdown
    # --------
    # Stop the background scheduler
    background_task_scheduler.shutdown()

app = FastAPI(title="LLM Fine-tuning API", lifespan=lifespan)

# Add SessionMiddleware for user authentication
app.add_middleware(SessionMiddleware, secret_key=config.app_secret_key, domain=config.base_domain_name)

# Add exception handlers
app.add_exception_handler(AppException, app_exception_handler)
app.add_exception_handler(RequestValidationError, validation_exception_handler)
app.add_exception_handler(SQLAlchemyError, sqlalchemy_exception_handler)
app.add_exception_handler(Exception, generic_exception_handler)

# Add routers
api_prefix = config.api_v1_prefix
app.include_router(users.router, prefix=api_prefix)
app.include_router(api_keys.router, prefix=api_prefix)
app.include_router(datasets.router, prefix=api_prefix)
app.include_router(fine_tuning.router, prefix=api_prefix)
app.include_router(models.router, prefix=api_prefix)
app.include_router(usage.router, prefix=api_prefix)
app.include_router(auth0.router, prefix=api_prefix)
app.include_router(billing.router, prefix=api_prefix)

# Set up Jinja2 HTML templates
templates = Jinja2Templates(directory="html")

@app.get("/", response_class=HTMLResponse)
async def web_console(request: Request):
    return templates.TemplateResponse("console.html", {"request": request})

@app.get(api_prefix + "/health")
async def health():
    return {"does it work?": "yes it does"}

if __name__ == "__main__":
    uvicorn.run("app.main:app", host="0.0.0.0", port=5100, reload=True)
